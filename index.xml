<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Engineer &amp; Software Developer in Boston</title>
    <link>http://andrewrgoss.com/</link>
    <description>Recent content on Data Engineer &amp; Software Developer in Boston</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>andrewrgoss@gmail.com (Andrew Goss)</managingEditor>
    <webMaster>andrewrgoss@gmail.com (Andrew Goss)</webMaster>
    <lastBuildDate>Thu, 23 Mar 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://andrewrgoss.com/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Getting Started With NSQ for Go</title>
      <link>http://andrewrgoss.com/2017/getting-started-with-nsq-for-go/</link>
      <pubDate>Thu, 23 Mar 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/getting-started-with-nsq-for-go/</guid>
      <description>

&lt;p&gt;&lt;a href=&#34;http://nsq.io&#34; target=&#34;_blank&#34;&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/nsq.png&#34; alt=&#34;NSQ&#34; title=&#34;NSQ&#34; /&gt;&lt;/a&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;ve been recently working on software application project at &lt;a href=&#34;https://www.digitaslbi.com/en-us&#34; target=&#34;_blank&#34;&gt;DigitasLBi&lt;/a&gt; where we adopted &lt;a href=&#34;http://nsq.io&#34; target=&#34;_blank&#34;&gt;NSQ&lt;/a&gt; as part of our microservices architecture. With this post I&amp;rsquo;d like to share how I got started using NSQ for &lt;a href=&#34;https://golang.org&#34; target=&#34;_blank&#34;&gt;Go&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-is-nsq:dda9787168a23dcd34b7e3f1e03bb799&#34;&gt;What is NSQ?&lt;/h2&gt;

&lt;p&gt;Per the official website, NSQ is a realtime distributed messaging platform designed to operate at scale, handling billions of messages per day.&lt;/p&gt;

&lt;p&gt;It promotes distributed and decentralized topologies without single points of failure, enabling fault tolerance and high availability coupled with a reliable message delivery guarantee.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://f.cloud.github.com/assets/187441/1700696/f1434dc8-6029-11e3-8a66-18ca4ea10aca.gif&#34;&gt;&lt;/img&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nsqd&lt;/code&gt; is the daemon that receives, queues, and delivers messages to clients.&lt;/p&gt;

&lt;p&gt;With the chart above you can understand the basic premise of NSQ involves producers and consumers - within your code you create NSQ producers to push messages to NSQ that get queued up to be consumed by other programs within your application.&lt;/p&gt;

&lt;h2 id=&#34;installing-nsq-on-linux-vm:dda9787168a23dcd34b7e3f1e03bb799&#34;&gt;Installing NSQ (on Linux VM)&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ wget https://s3.amazonaws.com/bitly-downloads/nsq/nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz
$ tar nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz
$ sudo mv nsq-1.0.0-compat.linux-amd64.go1.8.tar.gz/bin/* /usr/local/bin
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;launching-nsqd:dda9787168a23dcd34b7e3f1e03bb799&#34;&gt;Launching NSQD&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ nsqlookupd &amp;amp; 
$ nsqd --lookupd-tcp-address=127.0.0.1:4160 &amp;amp;
$ nsqadmin --lookupd-http-address=127.0.0.1:4161 &amp;amp;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If done successfully you will be able to view a web UI that looks like this:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://nsq.io/static/img/nsqadmin_screenshot.png&#34;&gt;&lt;/img&gt;&lt;/p&gt;

&lt;p&gt;&lt;code&gt;nsqadmin&lt;/code&gt; is a Web UI to view aggregated cluster stats in realtime and perform various administrative tasks.&lt;/p&gt;

&lt;h2 id=&#34;writing-your-go-program:dda9787168a23dcd34b7e3f1e03bb799&#34;&gt;Writing Your Go Program&lt;/h2&gt;

&lt;p&gt;To import the NSQ client library, use a &lt;code&gt;go get&lt;/code&gt; command:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ go get github.com/nsqio/go-nsq
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;creating-a-consumer:dda9787168a23dcd34b7e3f1e03bb799&#34;&gt;Creating a Consumer&lt;/h3&gt;

&lt;p&gt;I like creating the consumer first so I can see the handler in action after pushing a message with a producer (see next &lt;a href=&#34;#producer&#34;&gt;section&lt;/a&gt;).&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
    &amp;quot;log&amp;quot;
    &amp;quot;sync&amp;quot;

    &amp;quot;github.com/nsqio/go-nsq&amp;quot;
)

func main() {
    wg := &amp;amp;sync.WaitGroup{}
    wg.Add(1)

    decodeConfig := nsq.NewConfig()
    c, err := nsq.NewConsumer(&amp;quot;My NSQ Topic&amp;quot;, &amp;quot;My NSQ Channel&amp;quot;, decodeConfig)
    if err != nil {
        log.Panic(&amp;quot;Could not create consumer&amp;quot;)
    }
    //c.MaxInFlight defaults to 1

    c.AddHandler(nsq.HandlerFunc(func(message *nsq.Message) error {
        log.Println(&amp;quot;NSQ message received:&amp;quot;)
        log.Println(string(message.Body))
        return nil
    }))

    err = c.ConnectToNSQD(&amp;quot;127.0.0.1:4150&amp;quot;)
    if err != nil {
        log.Panic(&amp;quot;Could not connect&amp;quot;)
    }
    log.Println(&amp;quot;Awaiting messages from NSQ topic \&amp;quot;My NSQ Topic\&amp;quot;...&amp;quot;)
    wg.Wait()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now run this consumer program:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;$ go run main.go
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This should hang there waiting to receive a NSQ message from a topic you specify. Nothing will happen just yet since there aren&amp;rsquo;t any queued up messages for this particular topic. Leave this program running in a terminal window for now. In the next step we&amp;rsquo;ll push a message to it.&lt;/p&gt;

&lt;h3 id=&#34;a-name-producer-a-creating-a-producer:dda9787168a23dcd34b7e3f1e03bb799&#34;&gt;&lt;a name=&#34;producer&#34;&gt;&lt;/a&gt;Creating a Producer&lt;/h3&gt;

&lt;p&gt;You can publish a message with a producer with some simple code like this:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-go&#34;&gt;package main

import (
  &amp;quot;log&amp;quot;

  &amp;quot;github.com/nsqio/go-nsq&amp;quot;
)

func main() {
    config := nsq.NewConfig()
    p, err := nsq.NewProducer(&amp;quot;127.0.0.1:4150&amp;quot;, config)
    if err != nil {
        log.Panic(err)
    }
    err = p.Publish(&amp;quot;My NSQ Topic&amp;quot;, []byte(&amp;quot;sample NSQ message&amp;quot;))
    if err != nil {
        log.Panic(err)
    }
}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The Rise of the Data Engineer</title>
      <link>http://andrewrgoss.com/2017/the-rise-of-the-data-engineer/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/the-rise-of-the-data-engineer/</guid>
      <description>

&lt;p&gt;&lt;sub&gt;&lt;i&gt;written by &lt;a href=&#34;https://medium.freecodecamp.com/@maximebeauchemin&#34; target=&#34;_blank&#34;&gt;Maxime Beauchemin&lt;/a&gt;&lt;/i&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;I joined Facebook in 2011 as a &lt;b&gt;business intelligence engineer&lt;/b&gt;. By the time I left in 2013, I was a &lt;b&gt;data engineer&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;I wasn&amp;rsquo;t promoted or assigned to this new role. Instead, Facebook came to realize that the work we were doing transcended classic business intelligence. The role we&amp;rsquo;d created for ourselves was a new discipline entirely.&lt;/p&gt;

&lt;p&gt;My team was at forefront of this transformation. We were developing new skills, new ways of doing things, new tools, and&amp;ndash;more often than not&amp;ndash;turning our backs to traditional methods.&lt;/p&gt;

&lt;p&gt;We were pioneers. We were data engineers!&lt;/p&gt;

&lt;h2 id=&#34;data-engineering:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Data engineering?&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Data science&lt;/b&gt; as a discipline was going through its adolescence of self-affirming and defining itself. At the same time, &lt;b&gt;data engineering&lt;/b&gt; was the slightly younger sibling, but it was going through something similar. The data engineering discipline took cues from its sibling, while also defining itself in opposition, and finding its own identity.&lt;/p&gt;

&lt;p&gt;Like data scientists, data engineers write code. They&amp;rsquo;re highly analytical, and are interested in data visualization.&lt;/p&gt;

&lt;p&gt;Unlike data scientists&amp;ndash;and inspired by our more mature parent, &lt;b&gt;software engineering&lt;/b&gt;&amp;ndash;data engineers build tools, infrastructure, frameworks, and services. In fact, it&amp;rsquo;s arguable that data engineering is much closer to software engineering than it is to a data science.&lt;/p&gt;

&lt;p&gt;In relation to &lt;b&gt;previously existing roles&lt;/b&gt;, the data engineering field could be thought of as a superset of &lt;b&gt;business intelligence&lt;/b&gt; and &lt;b&gt;data warehousing&lt;/b&gt; that brings more elements from &lt;b&gt;software engineering&lt;/b&gt;. This discipline also integrates specialization around the operation of so called &amp;ldquo;big data&amp;rdquo; distributed systems, along with concepts around the extended Hadoop ecosystem, stream processing, and in computation at scale.&lt;/p&gt;

&lt;p&gt;In smaller companies&amp;ndash;where no &lt;b&gt;data infrastructure&lt;/b&gt; team has yet been formalized&amp;ndash;the data engineering role may also cover the workload around setting up and operating the organization&amp;rsquo;s data infrastructure. This includes tasks like setting up and operating platforms like Hadoop/Hive/HBase, Spark, and the like.&lt;/p&gt;

&lt;p&gt;In smaller environments people tend to use hosted services offered by Amazon or Databricks, or get support from companies like Cloudera or Hortonworks&amp;ndash;which essentially subcontracts the data engineering role to other companies.&lt;/p&gt;

&lt;p&gt;In larger environments, there tends to be specialization and the creation of a formal role to manage this workload, as the need for a data infrastructure team grows. In those organizations, the role of automating some of the data engineering processes falls under the hand of both the data engineering and data infrastructure teams, and it&amp;rsquo;s common for these teams to collaborate to solve higher level problems.&lt;/p&gt;

&lt;p&gt;While the engineering aspect of the role is growing in scope, other aspects of the original business engineering role are becoming secondary. Areas like crafting and maintaining portfolios of reports and dashboards are not a data engineer&amp;rsquo;s primary focus.&lt;/p&gt;

&lt;p&gt;We now have better self-service tooling where analysts, data scientist and the general &amp;ldquo;information worker&amp;rdquo; is becoming more data-savvy and can take care of data consumption autonomously.&lt;/p&gt;

&lt;h2 id=&#34;etl-is-changing:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;ETL is changing&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve also observed a general shift away from drag-and-drop &lt;a href=&#34;https://en.wikipedia.org/wiki/Extract,_transform,_load&#34; target=&#34;_blank&#34;&gt;ETL (Extract Transform and Load)&lt;/a&gt; tools towards a more programmatic approach. Product know-how on platforms like Informatica, IBM Datastage, Cognos, AbInitio or Microsoft SSIS isn&amp;rsquo;t common amongst modern data engineers, and being replaced by more generic software engineering skills along with understanding of programmatic or configuration driven platforms like Airflow, Oozie, Azkabhan or Luigi. It&amp;rsquo;s also fairly common for engineers to develop and manage their own job orchestrator/scheduler.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a multitude of reasons why complex pieces of software are not developed using drag and drop tools: it&amp;rsquo;s that &lt;mark&gt;ultimately &lt;b&gt;code&lt;/b&gt; is the best abstraction there is for software&lt;/mark&gt;. While it&amp;rsquo;s beyond the scope of this article to argue on this topic, it&amp;rsquo;s easy to infer that these same reasons apply to writing ETL as it applies to any other software. Code allows for arbitrary levels of abstractions, allows for all logical operation in a familiar way, integrates well with source control, is easy to version and to collaborate on. The fact that ETL tools evolved to expose graphical interfaces seems like a detour in the history of data processing, and would certainly make for an interesting blog post of its own.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s highlight the fact that the abstractions exposed by traditional ETL tools are off-target. Sure, there&amp;rsquo;s a need to abstract the complexity of data processing, computation and storage. But I would argue that the solution is not to expose ETL primitives (like source/target, aggregations, filtering) into a drag-and-drop fashion. The abstractions needed are of a higher level.&lt;/p&gt;

&lt;p&gt;For example, an example of a needed abstraction in a modern data environment is the configuration for the experiments in an A/B testing framework: &lt;i&gt;what are all the experiment? what are the related treatments? what percentage of users should be exposed? what are the metrics that each experiment expects to affect? when is the experiment taking effect?&lt;/i&gt; In this example, we have a framework that receives precise, high level input, performs complex statistical computation and delivers computed results. We expect that adding an entry for a new experiment will result in extra computation and results being delivered. What is important to note in this example is that the input parameters of this abstraction are not the one offered by a traditional ETL tool, and that a building such an abstraction in a drag and drop interface would not be manageable.&lt;/p&gt;

&lt;p&gt;To a modern data engineer, traditional ETL tools are largely &lt;b&gt;obsolete&lt;/b&gt; because logic cannot be expressed using code. As a result, the abstractions needed cannot be expressed intuitively in those tools. Now knowing that the data engineer&amp;rsquo;s role consist largely of defining ETL, and knowing that a completely new set of tools and methodology is needed, one can argue that this forces the discipline to rebuild itself from the ground up. New stack, new tools, a new set of constraints, and in many cases, a new generation of individuals.&lt;/p&gt;

&lt;h2 id=&#34;data-modeling-is-changing:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Data modeling is changing&lt;/h2&gt;

&lt;p&gt;Typical data modeling techniques&amp;ndash;like the &lt;a href=&#34;https://en.wikipedia.org/wiki/Star_schema&#34; target=&#34;_blank&#34;&gt;star schema&lt;/a&gt;&amp;ndash;which defined our approach to data modeling for the analytics workloads typically associated with data warehouses, are less relevant than they once were. The traditional best practices of data warehousing are loosing ground on a shifting stack. Storage and compute is cheaper than ever, and with the advent of distributed databases that scale out linearly, the scarcer resource is engineering time.&lt;/p&gt;

&lt;p&gt;Here are some changes observed in data modeling techniques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;b&gt;further denormalization:&lt;/b&gt; maintaining &lt;a href=&#34;http://www.kimballgroup.com/1998/05/surrogate-keys&#34; target=&#34;_blank&#34;&gt;surrogate keys&lt;/a&gt; in dimensions can be tricky, and it makes fact tables less readable. The use of natural, human readable keys and dimension attributes in fact tables is becoming more common, reducing the need for costly joins that can be heavy on distributed databases. Also note that support for encoding and compression in serialization formats like Parquet or ORC, or in database engines like Vertica, address most of the performance loss that would normally be associated with denormalization. Those systems have been taught to normalize the data for storage on their own.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;blobs:&lt;/b&gt; modern databases have a growing support for blobs through native types and functions. This opens new moves in the data modeler&amp;rsquo;s playbook, and can allow for fact tables to store multiple grains at once when needed&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;dynamic schemas:&lt;/b&gt; since the advent of map reduce, with the growing popularity of document stores and with support for blobs in databases, it&amp;rsquo;s becoming easier to evolve database schemas without executing &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_definition_language&#34; target=&#34;_blank&#34;&gt;DML&lt;/a&gt;. This makes it easier to have an iterative approach to warehousing, and removes the need to get full consensus and buy-in prior to development.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;systematically &lt;b&gt;snapshoting&lt;/b&gt; dimensions (storing a full copy of the dimension for each ETL schedule cycle, usually in distinct table partitions) as a generic way to handle &lt;a href=&#34;https://en.wikipedia.org/wiki/Slowly_changing_dimension&#34; target=&#34;_blank&#34;&gt;slowly changing dimension&lt;/a&gt; (SCD) is a simple generic approach that requires little engineering effort, and that unlike the classical approach, is easy to grasp when writing ETL and queries alike. It&amp;rsquo;s also easy and relatively cheap to denormalize the dimension&amp;rsquo;s attribute into the fact table to keep track of its value at the moment of the transaction. In retrospect, complex SCD modeling techniques are not intuitive and reduce accessibility.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;conformance&lt;/b&gt;, as in &lt;a href=&#34;http://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/conformed-dimension&#34; target=&#34;_blank&#34;&gt;conformed dimensions&lt;/a&gt; and metrics is still extremely important in modern data environment, but with the need for data warehouses to move fast, and with more team and roles invited to contribute to this effort, it&amp;rsquo;s less imperative and more of a tradeoff. Consensus and convergence can happen as a background process in the areas where the pain point of divergence become out-of-hand.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, more generally, it&amp;rsquo;s arguable to say that with the commoditization of compute cycles and with more people being data-savvy then before, there&amp;rsquo;s less need to precompute and store results in the warehouse. For instance you can have complex Spark job that can compute complex analysis on-demand only, and not be scheduled to be part of the warehouse.&lt;/p&gt;

&lt;h2 id=&#34;roles-responsibilities:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Roles &amp;amp; responsibilities&lt;/h2&gt;

&lt;h3 id=&#34;the-data-warehouse:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;The data warehouse&lt;/h3&gt;

&lt;p&gt;&lt;i&gt;A data warehouse is a copy of transaction data specifically structured for query and analysis.&lt;/i&gt;  &amp;ndash;Ralph Kimball&lt;/p&gt;

&lt;p&gt;&lt;i&gt;A data warehouse is a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management&amp;rsquo;s decision making process.&lt;/i&gt;  &amp;ndash;Bill Inmon&lt;/p&gt;

&lt;p&gt;The data warehouse is just as relevant as it ever was, and data engineers are in charge of many aspects of its construction and operation. The data engineer&amp;rsquo;s focal point is the data warehouse and gravitates around it.&lt;/p&gt;

&lt;p&gt;The modern data warehouse is a more public institution than it was historically, welcoming data scientists, analysts, and software engineers to partake in its construction and operation. Data is simply too centric to the company&amp;rsquo;s activity to have limitation around what roles can manage its flow. While this allows scaling to match the organization&amp;rsquo;s data needs, it often results in a much more chaotic, shape-shifting, imperfect piece of infrastructure.&lt;/p&gt;

&lt;p&gt;The data engineering team will often own pockets of certified, high quality areas in the data warehouse. At Airbnb for instance, there&amp;rsquo;s a set of &amp;ldquo;core&amp;rdquo; schemas that are managed by the data engineering team, where service level agreement (SLAs) are clearly defined and measured, naming conventions are strictly followed, business metadata and documentation is of the highest quality, and the related pipeline code follows a set of well defined best practices.&lt;/p&gt;

&lt;p&gt;It also becomes the role of the data engineering team to be a &amp;ldquo;center of excellence&amp;rdquo; through the definitions of standards, best practices and certification processes for data objects. The team can evolve to partake or lead an education program sharing its core competencies to help other teams become better citizens of the data warehouse. For instance, Facebook has a &amp;ldquo;data camp&amp;rdquo; education program and Airbnb is developing a similar &amp;ldquo;Data University&amp;rdquo; program where data engineers lead session that teach people how to be proficient with data.&lt;/p&gt;

&lt;p&gt;Data engineers are also the &amp;ldquo;librarians&amp;rdquo; of the data warehouse, cataloging and organizing metadata, defining the processes by which one files or extract data from the warehouse. In a fast growing, rapidly evolving, slightly chaotic data ecosystem, metadata management and tooling become a vital component of a modern data platform.&lt;/p&gt;

&lt;h3 id=&#34;performance-tuning-and-optimization:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Performance tuning and optimization&lt;/h3&gt;

&lt;p&gt;With data becoming more strategic than ever, companies are growing impressive budgets for their data infrastructure. This makes it increasingly rational for data engineers to spend cycles on performance tuning and optimization of data processing and storage. Since the budgets are rarely shrinking in this area, optimization is often coming from the perspective of achieving more with the same amount of resources or trying to linearize exponential growth in resource utilization and costs.&lt;/p&gt;

&lt;p&gt;Knowing that the complexity of the data engineering stack is exploding we can assume that the complexity of optimizing such stack and processes can be just as challenging. Where it can be easy to get huge wins with little effort, diminishing returns laws typically apply.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s definitely in the interest of the data engineer to build [on] infrastructure that scales with the company, and to be resource conscious at all times.&lt;/p&gt;

&lt;h4 id=&#34;data-integration:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Data Integration&lt;/h4&gt;

&lt;p&gt;Data integration, the practice behind integrating businesses and systems
through the exchange of data, is as important and as challenging as its ever
been. As &lt;a href=&#34;https://en.wikipedia.org/wiki/Software_as_a_service&#34; target=&#34;_blank&#34;&gt;Software as a Service (SaaS)&lt;/a&gt; becomes the new standard way for companies to operate, the need to synchronize referential data across these systems becomes increasingly critical. Not only SaaS needs up-to-date data to function, we often want to bring the data generated on their side into our data warehouse so that it can be analyzed along the rest of our data. Sure SaaS often have their own analytics offering, but are systematically lacking the perspective that the rest of you company&amp;rsquo;s data offer, so more often than not it&amp;rsquo;s necessary to pull some of this data back.&lt;/p&gt;

&lt;p&gt;Letting these SaaS offering redefine referential data without integrating and sharing a common primary key is a disaster that should be avoided at all costs. No one wants to manually maintain two employee or customer lists in 2 different systems, and worst: having to do fuzzy matching when bringing their HR data back into their warehouse.&lt;/p&gt;

&lt;p&gt;Worst, company executive often sign deal with SaaS providers without really
considering the data integration challenges. The integration workload is systematically downplayed by vendors to facilitate their sales, and leaves data engineers stuck doing unaccounted, under appreciated work to do. Let alone the fact that typical SaaS APIs are often poorly designed, unclearly documented and &amp;ldquo;agile&amp;rdquo;: meaning that you can expect them to change without notice.&lt;/p&gt;

&lt;h3 id=&#34;services:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Services&lt;/h3&gt;

&lt;p&gt;Data engineers are operating at a higher level of abstraction and in some cases that means providing services and tooling to automate the type of work that data engineers, data scientists or analysts may do manually.&lt;/p&gt;

&lt;p&gt;Here are a few examples of services that data engineers and data infrastructure engineer may build and operate.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;data ingestion: services and tooling around &amp;ldquo;scraping&amp;rdquo; databases, loading logs, fetching data from external stores or APIs, &amp;hellip;&lt;/li&gt;
&lt;li&gt;metric computation: frameworks to compute and summarize engagement, growth or segmentation related metrics&lt;/li&gt;
&lt;li&gt;anomaly detection: automating data consumption to alert people anomalous events occur or when trends are changing significantly&lt;/li&gt;
&lt;li&gt;metadata management: tooling around allowing generation and consumption of metadata, making it easy to find information in and around the data warehouse&lt;/li&gt;
&lt;li&gt;experimentation: A/B testing and experimentation frameworks is often a critical piece of company&amp;rsquo;s analytics with a significant data engineering component to it&lt;/li&gt;
&lt;li&gt;instrumentation: analytics starts with logging events and attributes related to those events, data engineers have vested interests in making sure that high quality data is captured upstream&lt;/li&gt;
&lt;li&gt;sessionization: pipelines that are specialized in understand series of actions in time, allowing analysts to understand user behaviors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just like software engineers, data engineers should be constantly looking to automate their workloads and building abstraction that allow them to climb the complexity ladder. While the nature of the workflows that can be automated differs depending on the environment, the need to automate them is common across the board.&lt;/p&gt;

&lt;h2 id=&#34;required-skills:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Required Skills&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;SQL mastery:&lt;/b&gt; if english is the language of business, SQL is the language of data. How successful of a business man can you be if you don&amp;rsquo;t speak good english? While generations of technologies age and fade, SQL is still standing strong as the lingua franca of data. A data engineer should be able to express any degree of complexity in SQL using techniques like &amp;ldquo;correlated subqueries&amp;rdquo; and window functions. SQL/DML/DDL primitives are simple enough that it should hold no secrets to a data engineer. Beyond the declarative nature of SQL, she/he should be able to read and understand database execution plans, and have an understanding of what all the steps are, how indices work, the different join algorithm and the distributed dimension within the plan.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Data modeling techniques:&lt;/b&gt; for a data engineer, entity-relationship modeling should be a cognitive reflex, along with a clear understanding of normalization, and have a sharp intuition around denormalization tradeoffs. The data engineer should be familiar with &lt;a href=&#34;https://en.wikipedia.org/wiki/Dimensional_modeling&#34; target=&#34;_blank&#34;&gt;dimensional modeling&lt;/a&gt; and the related concepts and lexical field.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ETL design:&lt;/b&gt; writing efficient, resilient and &amp;ldquo;evolvable&amp;rdquo; ETL is key. This topic will be expanded upon in an upcoming blog post.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Architectural projections:&lt;/b&gt; like any professional in any given field of expertise, the data engineer needs to have a high level understanding of most of the tools, platforms, libraries and other resources at its disposal. The properties, use-cases and subtleties behind the different flavors of databases, computation engines, stream processors, message queues, workflow orchestrators, serialization formats and other related technologies. When designing solutions, she/he should be able to make good choices as to which technologies to use and have a vision as to how to make them work together.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.freecodecamp.com/the-rise-of-the-data-engineer-91be18f1e603&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View original article&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Source: &lt;a href=&#34;https://medium.freecodecamp.com&#34; target=_&gt;FreeCodeCamp&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Lesson Transcripts/Labs&lt;/b&gt;&lt;br&gt;
&lt;hr&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_1_What_is_Hadoop_Part1&#34;&gt;Unit_1 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; What_is_Hadoop_Part1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_1_What_is_Hadoop_Part2&#34;&gt;Unit_1 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; What_is_Hadoop_Part2&lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Part1&#34;&gt;Unit_2 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; Hadoop_Architecture_Part1&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Part2&#34;&gt;Unit_2 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; Hadoop_Architecture_Part2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_HDFS_Command_Line&#34;&gt;Unit_2 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; HDFS_Command_Line&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Lab&#34;&gt;Unit_2 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; Hadoop_Architecture_Lab&lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_3_Hadoop_Administration&#34;&gt;Unit_3 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; Hadoop_Administration&lt;/a&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_MapReduce&#34;&gt;Unit_4 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; MapReduce&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_Pig_Hive&#34;&gt;Unit_4 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; Pig_Hive&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_Flume_Sqoop_Oozie&#34;&gt;Unit_4 &amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash;&amp;mdash; Flume_Sqoop_Oozie&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_1_What_is_Hadoop_Part1/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_1_What_is_Hadoop_Part1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_1_What_is_Hadoop_Part1&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/-65WgvIJ5xo&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Hello everyone and welcome to Hadoop Fundamentals What is Hadoop. My name is Asma Desai and I will be covering this topic.&lt;/p&gt;

&lt;p&gt;In this video we will explain what is Hadoop and what is Big Data. We will define some Hadoop-related open source projects and give some examples of Hadoop in action. Finally we will end off with some Big Data solutions and the Cloud.&lt;/p&gt;

&lt;p&gt;Imagine this scenario: You have 1GB of data that you need to process.&lt;/p&gt;

&lt;p&gt;The data is stored in a relational database in your desktop computer which has no problem handling the load. Then your company starts growing very quickly, and that data grows to 10GB, then 100GB, and you start to reach the limits of your current desktop computer.&lt;/p&gt;

&lt;p&gt;So what do you do? You scale up by investing in a larger computer, and you are then OK for a few more months. When your data grows from 1 TB to 10TB, and then 100TB, you are again quickly approaching the limits of that computer.&lt;/p&gt;

&lt;p&gt;Moreover, you are now asked to feed your application with unstructured data coming from sources like Facebook, Twitter, RFID readers, sensors, and so on. Your management wants to derive information from both the relational data and the unstructured data and wants this information as soon as possible. What should you do? Hadoop may be the answer.&lt;/p&gt;

&lt;p&gt;What is Hadoop? Hadoop is an open source project of the Apache Foundation. It is a framework written in Java originally developed by Doug Cutting who named it after his son&amp;rsquo;s toy elephant. Hadoop uses Google&amp;rsquo;s MapReduce technology as its foundation. It is optimized to handle massive quantities of data which could be structured, unstructured or semi-structured, using commodity hardware, that is, relatively inexpensive computers.&lt;/p&gt;

&lt;p&gt;This massive parallel processing is done with great performance. However, it is a batch operation handling massive amounts of data, so the response time is not immediate. Currently, in place updates are not possible in Hadoop, but appends to existing data is supported.&lt;/p&gt;

&lt;p&gt;Now, what&amp;rsquo;s the value of a system if the information it stores or retrieves is not consistent? Hadoop replicates its data across different computers, so that if one goes down, the data is processed on one of the replicated computers. Hadoop is not suitable for OnLine Transaction Processing workloads where data is randomly accessed on structured data like a relational database.&lt;/p&gt;

&lt;p&gt;Also, Hadoop is not suitable for OnLine Analytical Processing or Decision Support System workloads where data is sequentially accessed on structured data like a relational database, to generate reports that provide business intelligence. As of Hadoop version 2.6, updates are not possible, but appends are possible. Hadoop is used for Big Data. It complements OnLine Transaction Processing and OnLine Analytical Processing. It is NOT a replacement for a relational database system.&lt;/p&gt;

&lt;p&gt;So, what is Big Data? With all the devices available today to collect data, such as RFID readers, microphones, cameras, sensors, and so on, we are seeing an explosion in data being collected worldwide. Big Data is a term used to describe large collections of data (also known as datasets) that may be unstructured, and grow so large and quickly that it is difficult to manage with a regular database or statistical tools.&lt;/p&gt;

&lt;p&gt;In terms of numbers, what are we looking at? How BIG is &amp;ldquo;big data&amp;rdquo;? Well there are more than 3.2 billion internet users, and active cell phones have surpassed 7.6 billion. There are now more in-use cell phones than there are people on the planet (7.4 billion).&lt;/p&gt;

&lt;p&gt;Twitter processes 7TB of data ever day, and 600TB of data is processed by Facebook every day. Interestingly, about 80% of this data is unstructured. With this massive amount of data, businesses need fast, reliable, deeper data insight. Therefore, Big Data solutions based on Hadoop and other analytics software are becomingmore and more relevant.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_1_What_is_Hadoop_Part2/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_1_What_is_Hadoop_Part2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_1_What_is_Hadoop_Part2&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/PS5QSGAoLNw&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;This is a list of some other open source project related to Hadoop:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Eclipse is a popular IDE donated by IBM to the open-source community&lt;/li&gt;
&lt;li&gt;Lucene is a text search engine library written in Java&lt;/li&gt;
&lt;li&gt;Hbase is a Hadoop database&lt;/li&gt;
&lt;li&gt;Hive provides data warehousing tools to extract, transform and load (ETL) data, and query this data stored in Hadoop files&lt;/li&gt;
&lt;li&gt;Pig is a high level language that generates MapReduce code to analyze large data sets.&lt;/li&gt;
&lt;li&gt;Spark is a cluster computing framework&lt;/li&gt;
&lt;li&gt;ZooKeeper is a centralized configuration service and naming registry for large distributed systems&lt;/li&gt;
&lt;li&gt;Ambari manages and monitors Hadoop clusters through an intuitive web UI&lt;/li&gt;
&lt;li&gt;Avro is a data serialization system&lt;/li&gt;
&lt;li&gt;UIMA is the architecture for the development, discovery, composition and deployment for the analysis of unstructured data&lt;/li&gt;
&lt;li&gt;Yarn is a large-scale operating system for big data applications&lt;/li&gt;
&lt;li&gt;Mapreduce is a software framework for easily writing applications which processes vast amounts of data&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Let&amp;rsquo;s now talk about examples of Hadoop in action.&lt;/p&gt;

&lt;p&gt;Early in 2011, Watson, a super computer developed by IBM competed in the popular Question and Answer show Jeopardy!. In that contest, Watson was successful in beating the two most winning Jeopardy players. Approximately 200 million pages of text were input using Hadoop to distribute the workload for loading this information into memory.&lt;/p&gt;

&lt;p&gt;Once this information was loaded, Watson used other technologies for advanced search and analysis. In the telecommunication industry we have China Mobile, a company that built a Hadoop cluster to perform data mining on Call Data Records. China Mobile was producing 5-8 TB of these records daily. By using a Hadoop-based system they were able to process 10 times as much data as when using their old system, and at one fifth the cost.&lt;/p&gt;

&lt;p&gt;In the media we have the New York Times which wanted to host on their website all public domain articles from 1851 to 1922. They converted articles from 11 million image files (4TB) to 1.5TB of PDF documents. This was implemented by one employee who ran a job in 24 hours on a 100-instance Amazon EC2 Hadoop cluster at a very low cost.&lt;/p&gt;

&lt;p&gt;In the technology field we again have IBM with IBM ES2, and enterprise search technology based on Hadoop, Nutch, Lucene and Jaql. ES2 is designed to address unique challenges of enterprise search such as: - The Use of enterprise-specific vocabulary, abbreviations and acronyms ES2 can perform mining tasks to build Acronym libraries, Regular expression patterns, and Geo-classification rules.&lt;/p&gt;

&lt;p&gt;There are also many internet or social network companies using Hadoop such as: Yahoo, Facebook, Amazon, eBay, Twitter, StumbleUpon, Rackspace, Ning, AOL, etc. Yahoo of course is the largest production user with an application running a Hadoop cluster consisting of about 10,000 Linux machines. Yahoo is also the largest contributor to the Hadoop open source project.&lt;/p&gt;

&lt;p&gt;Now, Hadoop is not a magic bullet that solves all kinds of problems. Hadoop is not good to process transactions due to its lack random access. It is not good when the work cannot be parallelized or when there are dependencies within the data, that is, record one must be processed before record two. It is not good for low latency data access. Not good for processing lots of small files although there is work being done in this area, for example, IBM&amp;rsquo;s Adaptive MapReduce. And it is not good for intensive calculations with little data.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s move on, and talk about Big Data solutions. Big Data solutions are more than just Hadoop. They can integrate analytic solutions to the mix to derive valuable information that can combine structured legacy data with new unstructured data.&lt;/p&gt;

&lt;p&gt;Big data solutions may also be used to derive information from data in motion, for example, IBM has a product called InfoSphere Streams that can be used to quickly determine customer sentiment for a new product based on Facebook or Twitter comments.&lt;/p&gt;

&lt;p&gt;Finally we would like to end this presentation with one final thought: Cloud computing has gained a tremendous track in the past few years, and it is a perfect fit for Big Data solutions. Using the cloud, a Hadoop cluster can be setup in minutes, on demand, and it can run for as long as needed without having to pay for more than what is used.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_HDFS_Command_Line/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_HDFS_Command_Line/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_2_HDFS_Command_Line&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/Gd1sVPOYzuk&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Welcome to HDFS command line interface.
In this presentation, I will cover the general usage of the HDFS command line
interface and commands specific to HDFS. Other commands should be familiar to
anyone with UNIX experience and will not be covered.&lt;/p&gt;

&lt;p&gt;The HDFS can be manipulated through a Java API or through a command line
interface. All commands for manipulating HDFS through Hadoop&amp;rsquo;s command line
interface begin with &lt;code&gt;hdfs&lt;/code&gt;, a space, and &lt;code&gt;dfs&lt;/code&gt;. This is the file system shell. This
is followed by the command name as an argument
to &lt;code&gt;hdfs dfs&lt;/code&gt;. These commands start with a dash. For example, the &lt;code&gt;ls&lt;/code&gt; command
for listing a directory is a common UNIX command and is preceded with a dash.
As on UNIX systems, &lt;code&gt;ls&lt;/code&gt; can take a path as an argument. In this example, the
path is the current directory, represented by a single dot.&lt;/p&gt;

&lt;p&gt;As we saw for the &lt;code&gt;ls&lt;/code&gt; command, the file system shell commands can take paths as arguments.
These paths can be expressed in the form of &lt;b&gt;uniform resource identifiers&lt;/b&gt; or URIs. The
URI format consists of a scheme, an authority, and path. There are multiple schemes supported.
The local file system has a scheme of &amp;ldquo;file&amp;rdquo;. HDFS has
a scheme called &amp;ldquo;hdfs&amp;rdquo;. For example, let us say you wish to copy a file called &amp;ldquo;myfile.txt&amp;rdquo;
from your local filesystem to an HDFS file system on the localhost. You can do this by
issuing the command shown. The cp command takes a URI for the source and a URI for the
destination. The scheme and the authority do not always need to be specified. Instead
you may rely on their default values. These defaults can be overridden by specifying them
in a file named core-site.xml in the conf directory of your Hadoop installation.
HDFS is not a fully POSIX compliant file system,
but it supports many of the commands. The HDFS commands are mostly easily-recognized
UNIX commands like &lt;code&gt;cat&lt;/code&gt; and &lt;code&gt;chmod&lt;/code&gt;. There are also a few commands that are specific to HDFS
such as &lt;code&gt;copyFromLocal&lt;/code&gt;. We&amp;rsquo;ll examine a few of these.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;copyFromLocal&lt;/code&gt; and &lt;code&gt;put&lt;/code&gt; are two HDFS-specific commands that do the same thing -
copy files from the local filesystem to a location on another filesystem. Their opposite
is the &lt;code&gt;copyToLocal&lt;/code&gt; command which can also be referred to as get. This command copies
files out of the filesystem you specify and into the local filesystem.
&lt;code&gt;getMerge&lt;/code&gt; is an enhanced form of get that can merge the files from multiple locations
into a single local file. &lt;code&gt;setRep&lt;/code&gt; lets you override the default level
of replication. You can do this for one file or, with the &lt;code&gt;-R&lt;/code&gt; option, to an entire tree.&lt;/p&gt;

&lt;p&gt;This command returns immediately after requesting the new replication level. If you want the
command to block until the job is done, pass the &lt;code&gt;-w&lt;/code&gt; option.
IBM, with BigInsights, provides the Ambari Console as graphical way to work with HDFS.
The services tab provides a simple way to view the status of the Hadoop components.
Create a file view to browse and work with directories and files.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Lab/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Lab/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_2_Hadoop_Architecture_Lab&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/35ZKvSjAoSo&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;&lt;a href=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/Unit_2_Hadoop_Architecture_Lab.pdf&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;Lab exercises&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Part1/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Part1/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_2_Hadoop_Architecture_Part1&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/8AtrYcqO5ho&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Welcome to the unit of Hadoop Fundamentals on Hadoop architecture.
I will begin with a terminology review and then cover the major components
of Hadoop. We will see what types of nodes can exist in a Hadoop cluster and talk about
how Hadoop uses replication to lessen data loss. Finally I will explain an important
feature of Hadoop called &amp;ldquo;rack awareness&amp;rdquo; or &amp;ldquo;network topology awareness&amp;rdquo;.&lt;/p&gt;

&lt;p&gt;Before we examine Hadoop components and architecture, let&amp;rsquo;s review some of the
terms that are used in this discussion. A node is simply a computer. This is typically
non-enterprise, commodity hardware for nodes that contain data. So in this example,
we have Node 1. Then we can add more nodes, such as Node 2, Node 3, and so on.
This would be called a rack. A rack is a collection of 30 or 40 nodes that are
physically stored close together and are all connected to the same network switch.
Network bandwidth between any two nodes in the same rack is greater than bandwidth
between two nodes on different racks. You will see later how Hadoop takes advantage
of this fact. A Hadoop Cluster (or just cluster from
now on) is a collection of racks.&lt;/p&gt;

&lt;p&gt;Let us now examine the pre-Hadoop 2.2 architecture. Hadoop has two major components:
- the distributed filesystem component, the main example of which is the Hadoop
Distributed File System, though other file systems, such as IBM Spectrum Scale, are supported.
- the MapReduce component, which is a framework for performing calculations on
the data in the distributed file system. Pre-Hadoop 2.2 MapReduce is referred to as MapReduce
V1 and has its own built-in resource manager and schedule. This unit covers the Hadoop
Distributed File System and MapReduce is covered separately.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s now examine the Hadoop distributed file system - HDFS
HDFS runs on top of the existing file systems on each node in a Hadoop cluster. It is not
POSIX compliant. It is designed to tolerate high component failure rate through replication
of the data. Hadoop works best with very large files. The
larger the file, the less time Hadoop spends seeking for the next data location
on disk, the more time Hadoop runs at the limit of the bandwidth of your disks.
Seeks are generally expensive operations that are useful when they only need to analyze
a small subset of your dataset. Since Hadoop is designed to run over your entire
dataset, it is best to minimize seeks by using large files. Hadoop is designed for
streaming or sequential data access rather than random access. Sequential data access
means fewer seeks, since Hadoop only seeks to the beginning of each block and begins
reading sequentially from there. Hadoop uses blocks to store a file or parts
of a file. This is shown in the figure.&lt;/p&gt;

&lt;p&gt;Let us now examine file blocks in more detail. A Hadoop block is a file on the underlying
filesystem. Since the underlying filesystem stores files as blocks, one Hadoop block may
consist of many blocks in the underlying file system. Blocks are large. They default to
64 megabytes each and most systems run with block sizes of 128 megabytes or larger. Blocks
have several advantages: Firstly, they are fixed in size. This makes
it easy to calculate how many can fit on a disk.&lt;/p&gt;

&lt;p&gt;Secondly, by being made up of blocks that can be spread over multiple nodes, a file
can be larger than any single disk in the cluster. HDFS blocks also don&amp;rsquo;t waste space.
If a file is not an even multiple of the block size, the block containing the remainder does
not occupy the space of an entire block. As shown in the figure, a 450 megabyte file with
a 128 megabyte block size consumes four blocks, but the fourth block does not consume a full
128 megabytes. Finally, blocks fit well with replication,
which allows HDFS to be fault tolerant and available on commodity hardware.
As shown in the figure: Each block is replicated to multiple nodes. For example, block 1 is
stored on node 1 and node 2. Block 2 is stored on node 1 and node 3. And block 3 is stored
on node 2 and node 3. This allows for node failure without data loss. If node 1 crashes,
node 2 still runs and has block 1&amp;rsquo;s data. In this example, we are only replicating data
across two nodes, but you can set replication to be across many more nodes by changing Hadoop&amp;rsquo;s
configuration or even setting the replication factor for each individual file.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/hdfs_replication.png&#34; alt=&#34;HDFS Replication&#34; title=&#34;HDFS Replication&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The second major component of Hadoop, described in detail in another lecture, is the MapReduce
component. HDFS was based on a paper Google published about their Google File System,
Hadoop&amp;rsquo;s MapReduce is inspired by a paper Google published on the MapReduce technology.
It is designed to process huge datasets for certain kinds of distributable problems using
a large number of nodes. A MapReduce program consists of two types of transformations that
can be applied to data any number of times - a map transformation and a reduce transformation.
A MapReduce job is an executing MapReduce program that is divided into map tasks that
run in parallel with each other and reduce tasks that run in parallel with each other.
Let us examine the main types of nodes in pre-Hadoop 2.2. They are classified as HDFS
or MapReduce V1 nodes. For HDFS nodes we have the NameNode, and the DataNodes. For MapReduce
V1 nodes we have the JobTracker and the TaskTracker nodes. Each of these is discussed in more
detail later in this presentation. There are other HDFS nodes such as the Secondary NameNode,
Checkpoint node, and Backup node that are not discussed in this course. This diagram
shows some of the communication paths between the different types of nodes on the system.
A client is shown as communicating with a JobTracker. It can also communicate with the
NameNode and with any DataNode.&lt;/p&gt;

&lt;p&gt;There is only one NameNode in the cluster. While the data that makes up a file is stored
in blocks at the data nodes, the metadata for a file is stored at the NameNode. The
NameNode is also responsible for the filesystem namespace. To compensate for the fact that
there is only one NameNode, one should configure the NameNode to write a copy of its state
information to multiple locations, such as a local disk and an NFS mount. If there is
one node in the cluster to spend money on the best enterprise hardware for maximum reliability,
it is the NameNode. The NameNode should also have as much RAM as possible because it keeps
the entire filesystem metadata in memory.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/types_of_nodes.png&#34; alt=&#34;Types of Nodes&#34; title=&#34;Types of Nodes&#34; /&gt;&lt;/p&gt;

&lt;p&gt;A typical HDFS cluster has many DataNodes. DataNodes store the blocks of data and blocks
from different files can be stored on the same DataNode. When a client requests a file,
the client finds out from the NameNode which DataNodes stored the blocks that make up that
file and the client directly reads the blocks from the individual DataNodes. Each DataNode
also reports to the NameNode periodically with the list of blocks it stores. DataNodes
do not require expensive enterprise hardware or replication at the hardware layer. The
DataNodes are designed to run on commodity hardware and replication is provided at the
software layer.&lt;/p&gt;

&lt;p&gt;A JobTracker node manages MapReduce V1 jobs. There is only one of these on the cluster.
It receives jobs submitted by clients. It schedules the Map tasks and Reduce tasks on
the appropriate TaskTrackers, that is where the data resides, in a rack-aware manner and
it monitors for any failing tasks that need to be rescheduled on a different
TaskTracker. To achieve the parallelism for your map and reduce tasks, there are many
TaskTrackers in a Hadoop cluster. Each TaskTracker spawns Java Virtual Machines to run your map
or reduce task. It communicates with the JobTracker and reads blocks from DataNodes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Part2/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_2_Hadoop_Architecture_Part2/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_2_Hadoop_Architecture_Part2&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/iJmJhxIsmb8&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Hadoop 2.2 brought about architectural changes to MapReduce. As Hadoop has matured, people
have found that it can be used for more than running MapReduce jobs. But to keep each new
framework from having its own resource manager and scheduler, that would compete with the
other framework resource managers and schedulers, it was decided to have the recourse manager
and schedulers to be external to any framework. This new architecture is called YARN. (Yet
Another Resource Negotiator) . You still have DataNodes but there are no longer TaskTrackers
and the JobTracker. You are not required to run YARN with Hadoop 2.2. as MapReduce V1 is
still supported.&lt;/p&gt;

&lt;p&gt;There are two main ideas with YARN. Provide generic scheduling and resource management. This way Hadoop can support more than just
MapReduce. The other is to try to provide a more efficient scheduling and workload management.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/yarn.png&#34; alt=&#34;YARN&#34; title=&#34;YARN&#34; /&gt;&lt;/p&gt;

&lt;p&gt;With MapReduce V1, the administrator had to define how many
map slots and how many reduce slots there were on each node. Since the hardware capabilities
for each node in a Hadoop cluster can vary, for performance reasons, you might want to
limit the number of tasks on certain nodes. With YARN, this is no longer required.
With YARN, the resource manager is aware of the capabilities of each node via communication
with the NodeManager running on each node. When an application gets invoked , an Application
Master gets started. The Application Master is then responsible for negotiating resources
from the ResourceManager. These resources are assigned to Containers on each slave-node
and you can think that tasks then run in Containers. With this architecture, you are no longer
forced into a one size fits all. The NameNode is a single point of failure.&lt;/p&gt;

&lt;p&gt;Is there anything that can be done about that? Hadoop now supports high availability. In
this setup, there are now two NameNodes, one active and one standby.&lt;/p&gt;

&lt;p&gt;Also, now there are JournalNodes. There must be at least three and there must be an odd
number. Only one of the NameNodes can be active at a time. It is the JournalNodes, working
together , that decide which of the NameNodes is to be the active one and if the active
NameNode has been lost and whether the backup NameNode should take over.&lt;/p&gt;

&lt;p&gt;The NameNode loads the metadata for the file system into memory. This is the reason that
we said that NameNodes needed large amounts of RAM. But you are going to be limited at
some point when you use this vertical growth model. Hadoop Federation allows you to grow
your system horizontally. This setup also utilizes multiple NameNodes. But they act
independently. However, they do all share all of the DataNodes. Each NameNode has its
own namespace and therefore has control over its own set of files. For example, one file
that has blocks on DataNode 1 and DataNode 2 might be owned by NameNode 1. NameNode 2
might own a file that has blocks on DataNode 2 and DataNode 3. And NameNode 3 might have
a file with blocks on all three DataNodes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/hadoop_federation.png&#34; alt=&#34;Hadoop Federation&#34; title=&#34;Hadoop Federation&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Hadoop has awareness of the topology of the network. This allows it to optimize where
it sends the computations to be applied to the data. Placing the work as close as possible
to the data it operates on maximizes the bandwidth available for reading the data. In the diagram,
the data we wish to apply processing to is block B1, the dark blue rectangle on node
n1 on rack 1. When deciding which TaskTracker should receive a MapTask that reads data from
B1, the best option is to choose the TaskTracker that runs on the same node as the data. If
we can&amp;rsquo;t place the computation on the same node, our next best option is to place it
on a node in the same rack as the data. The worst case that Hadoop currently supports
is when the computation must be processed from a node in a different rack than the data.
When rack-awareness is configured for your cluster, Hadoop will always try to run the
task on the TaskTracker node with the highest bandwidth access to the data.&lt;/p&gt;

&lt;p&gt;Let us walk through an example of how a file gets written to HDFS. First, the client submits
a &amp;ldquo;create&amp;rdquo; request to the NameNode. The NameNode checks that the file does not already exist
and the client has permission to write the file. If that succeeds, the NameNode determines
the DataNode to where the first block is to be written. If the client is running on a
DataNode, it will try to place it there. Otherwise, it chooses DataNode at random. By default,
data is replicated to two other places in the cluster. A pipeline is built between the
three DataNodes that make up the pipeline. The second DataNode is a randomly chosen node
on a rack other than that of the first replica of the block. This is to increase redundancy.
The final replica is placed on a random node within the same rack as the second replica.
The data is piped from the second DataNode to the third. To ensure the write was successful
before continuing, acknowledgment packets are sent from the third DataNode to the second,
From the second DataNode to the first And from the first DataNode to the client This
process occurs for each of the blocks that makes up the file Notice that, for every block,
by default, there is a replica on at least two racks. When the client is done writing
to the DataNode pipeline and has received acknowledgements, it tells the NameNode that
the write has completed. The NameNode then checks that the blocks are at least minimally
replicated before responding.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_3_Hadoop_Administration/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_3_Hadoop_Administration/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_3_Hadoop_Administration&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/LtGliUam-_U&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Welcome to the unit on Hadoop Administration. The agenda covers adding nodes to a cluster,
verifying the health of a cluster, and stopping / starting components. Then the unit covers
configuring Hadoop and setting up rack topology.
Let&amp;rsquo;s begin with adding and removing nodes from a cluster.&lt;/p&gt;

&lt;p&gt;Adding nodes can be performed from the Ambari Console. To do so requires either the ip address
or hostname of the node to be added. The node to be added must also be reachable. And as
a matter of fact, it works both ways. The master and child nodes must all be able to
communicate with each other. In this case, a child node refers to the node that is being
added. It may not have BigInsights already installed on it. When the node is added to
a cluster, the BigInsights code is transferred to the new node and installed.
From the Ambari Console you navigate to the Hosts tab and, on the left side, under Actions
select Add New Hosts. You are then presented with a dialog that allows you to specify one
or more nodes to be added. You may type in ip address, hostnames or any combination thereof.
You can even specify an ipaddress range or a regular expression with your hostname.
Once the nodes have been added, you define which services are to be hosted on those nodes.
You can select multiple services for one or more nodes.
Services can also be removed from a node.&lt;/p&gt;

&lt;p&gt;As a matter of fact, if you are using the Ambari Console to remove a node, you must
first remove all services from that node. Depending on which services are running on
a node, you select which are to be removed. When there are no services running on a node,
it can be removed using the Ambari Console.
Next let us discuss verifying the heath of your cluster.
You are able to view all of the nodes in the cluster, see the status of each node and which
services are running on each node.
From the command line you can run the DFS Disk Check report. This lets you see how
much space is still available on each DataNode.&lt;/p&gt;

&lt;p&gt;Next let us look at start and stopping components and services.
In order to save some resources, you may only want to start a subset of the Hadoop
components . Using Ambari, navigate to the Services tab and choose a service from the
left that you would like to stop or start. When the services main page appears on the
right side under Service Actions you can start or stop that service.
All services can be stopped or started from the main Dashboard
Now let us look at how to configure Hadoop.&lt;/p&gt;

&lt;p&gt;Hadoop is configured using a number of XML files. And each file controls a number of
parameters. There are three main configuration files with which you will work.
core-site.xml is used to configure the parameters that are common to both HDFS and
MapReduce. hdfs-site.xml contains parameters that
are for the HDFS daemons, like the NameNode and DataNodes.
mapred-site.xml controls the settings for MapReduce daemons, JobTracker and TaskTrackers.
We are not going to spend the time covering all of the configuration files. That would
just take too much time. However, you do have the option of pausing this video if you would
like to read the descriptions of the other configuration files.&lt;/p&gt;

&lt;p&gt;The hadoop-env.sh is a script that sets a number of environment variables. Normally,
with Hadoop, these variables are not set but with BigInsights, they are. There is one that
must always be set and that is the &lt;code&gt;JAVA_HOME&lt;/code&gt; environment variable.
Here are some of the settings found in core-site.xml. We are not going to spend time on these nor
those on this page as well. If you want to pause the video to read their description,
feel free to do so.&lt;/p&gt;

&lt;p&gt;Next we have some setting in hdfs-site.xml. If you want to set a different value for the
default block size, then you would modify dfs.block.size. Likewise, if you want to change
the default replication factor, then you would modify dfs.replication. Once again, I am not
going to cover all the parameters.&lt;/p&gt;

&lt;p&gt;To change MapReduce settings, you modify mapred-site.xml. You can control which nodes can connect to
the JobTracker. Mapreduce.job.reduces lets you set the number
of reduce tasks per job. mapreduce.map.speculative. execution allows the JobTracker, when having
determined that there might be a problem with one map task, to start another map task running
in parallel. Both map tasks process the same data and, upon successful completion of one
of the tasks, the other is terminated. mapreduce.tasktracker.map.tasks.maximum and mapreduce.tasktracker.reduce.tasks.maximum
lets you define the number of slots on a TaskTracker that can run map and reduce task.
mapreduce.jobtracker.taskScheduler points to the scheduler that is to be used for MapReduce
jobs.&lt;/p&gt;

&lt;p&gt;So how do you set these parameters? First of all, you must stop the appropriate service
or services before making the change. You are making changes to value element for the
appropriate property element. The configuration files are in the hadoop-client/conf directory.
The changes must be made to the configuration files on all nodes in the cluster.
Let me spend a few minutes and focus on BigInsights. With BigInsights the hadoop-conf directory
is under &lt;code&gt;$BIGINSIGHTS_HOME&lt;/code&gt;. But, and this is very important, you do not make changes
to the configuration files in that directory. BigInsights has a staging directory which
is &lt;code&gt;$BIGINSIGHTS_HOME/hdm/hadoop-conf-staging&lt;/code&gt; that has copies of the configuration files.
You make changes to the files in this staging directory and then execute a script that distributes
the changes to all nodes in the cluster.&lt;/p&gt;

&lt;p&gt;Finally, let us talk about setting up rack topology.
To make Hadoop aware of the clusters topology, you code a script that receives as arguments,
one or more ip addresses of nodes in the cluster. The script returns on stdout, a list of rack
names, one for each input value. Then you update core-site.xml and modify the topology.script.file.name
property to point to your script.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_Flume_Sqoop_Oozie/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_Flume_Sqoop_Oozie/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_4_Flume_Sqoop_Oozie&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/VL89pHMU1lw&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Now let us look at moving data into Hadoop. We will begin by looking at Flume&amp;rsquo;s architecture,
then examining the three modes it can run in followed by a look at the event data model.
Flume is an open source software program developed by Cloudera that acts as a service for aggregating
and moving large amounts of data around a Hadoop cluster as the data is produced or
shortly thereafter. Its primary use case is the gathering of log files from all the machines
in a cluster to persist them in a centralized store such as HDFS.&lt;/p&gt;

&lt;p&gt;This topic is not intended to cover all aspects of Sqoop but to give you an idea of the capabilities
of Sqoop. Sqoop is an open source product designed to
transfer data between relational database systems and Hadoop. It uses JDBC to access
the relational systems. Sqoop accesses the database in order to understand the schema
of the data. It then generates a MapReduce application to import or export the data.
When you use Sqoop to import data into Hadoop, Sqoop generates a Java class that encapsulates
one row of the imported table. You have access to the source code for the generate class.
This can allow you to quickly develop any other MapReduce applications that use the
records that Sqoop stored into HDFS.&lt;/p&gt;

&lt;p&gt;In Flume, you create data flows by building
up chains of logical nodes and connecting them to sources and sinks. For
example, say you wish to move data from an Apache access log into HDFS. You create
a source by tailing access.log and use a logical node to route this to an HDFS sink.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/flume_architecture_1.png&#34; alt=&#34;Flume Architecture 1&#34; title=&#34;Flume Architecture 1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Most production Flume deployments have a three tier design. The agent tier consists
of Flume agents co-located with the source of the data that is to be moved. The
collector tier consists of perhaps multiple collectors each of which collects data
coming in from multiple agents and forwards it on to the storage tier which may
consist of a file system such as HDFS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/flume_architecture_2.png&#34; alt=&#34;Flume Architecture 2&#34; title=&#34;Flume Architecture 2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Here is an example of such a design. Say we
have four http server nodes producing log files labelled httpd_logx where x is a
number between 1 and 4. Each of these http server nodes has a Flume agent process
running on it. There are two collector nodes. Collector1 is configured to take data
from Agent1 and Agent2 and route it to HDFS. Collector2 is configured to take data
from Agent3 and Agent4 and route it to HDFS. Having multiple collector processes
allows one to increase the parallelism in which data flows into HDFS from the web servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/flume_architecture_3.png&#34; alt=&#34;Flume Architecture 3&#34; title=&#34;Flume Architecture 3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Oozie is an open source job control component used to manage Hadoop jobs.
Oozie workflows are collections of actions arranged in a Direct Acyclic Graph. There
is a control dependency between actions in that a second action cannot run until the
proceeding action completes. For example, you have the capability of having one job
execute only when a previous job completed successfully. You can specify that several
jobs are permitted to execute in parallel but a final job must wait to begin executing
until all parallel jobs complete. Workflows are written in hPDL an XML process definition
language, and are stored in a file called workflow.xml.&lt;/p&gt;

&lt;p&gt;Each workflow action starts jobs in some remote system and that remote system performs a callback
to Oozie to notify that the action completed. The coordinator component can invoke workflows
based upon a time interval, that is for example, once every 15 minutes, or based upon the availability
of specific data. It might also be necessary to connect workflow jobs that run regularly
but at different time intervals. For example, you may want the output of the last four jobs
that run every 15 minutes to be the input to a job that runs every hour.
A single workflow can invoke a single task or multiple tasks, either in sequence or based
upon some control logic.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/oozie_coordinator.png&#34; alt=&#34;Oozie Coordinator&#34; title=&#34;Oozie Coordinator&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_MapReduce/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_MapReduce/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_4_MapReduce&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/sVrSx4zt8ho&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Welcome to Hadoop Fundamentals, Hadoop Components. In this unit I will discuss the MapReduce
Philosophy, describe the usage of Pig and Hive, talk about how to get data into Hadoop
through the use of Flume and Sqoop and finally describe how to schedule and control job execution
using Oozie.&lt;/p&gt;

&lt;p&gt;First I need to set the boundaries for this unit. The components presented in this unit
are done so at a very high level. The Hadoop environment is littered with a number of open
source components with funny sounding names. And to some people, it is difficult to understand
their usage. This unit is merely an attempt to provide you with descriptions of some of
these components. If you are interested in getting more detail about each of the components
covered in this unit, then I would direct you to the other Big Data University courses
that are specific to these components.&lt;/p&gt;

&lt;p&gt;Let us take a look at MapReduce. MapReduce is designed to process very large datasets for
certain types of distributable problems.
It attempts to spread the work across a large number of nodes and allows those nodes to
process the data in parallel. You cannot have dependencies within the data, meaning that
you cannot have a requirement that one record in a dataset must be processed before another.
Results from the initial parallel processing are sent to additional nodes where the data
is combined to allow for further reductions of the data.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s take a look at how the map and reduce operations working sequence on your
data to produce the final output. In this case, we will have a job with a single map
step and a single reduce step. The first step is the map step. It takes a subset of the
full dataset called an input split and applies to each row in the input split an operation
that you have written, such as parsing each character string.
The output data is buffered in memory and spills to disk.
It is sorted and partitioned by key using the default partitioner.
A merge sort sorts each partition. There may be multiple map operations running
in parallel with each other, each one processing a different input split.
The partitions are shuffled among the reducers. For example, partition 1 goes to reducer 1.
The second map task also sends its partition 1 to reducer 1.
Partition 2 goes to another reducer.
Additional map tasks would act in the same way.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/mapreduce_distributed_mergesort_engine.png&#34; alt=&#34;MapReduce - Distributed Mergesort Engine&#34; title=&#34;MapReduce - Distributed Mergesort Engine&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Each reducer does its own merge steps and executes the code of your reduce task.
For example, it could do a sum on the number of occurrences of a particular character string.
This produces sorted output at each reducer. The data that flows into and out of the mappers
and reducers takes a specific form. Data enters Hadoop in unstructured form but before it
gets to the first mapper, Hadoop has changed it into key-value pairs
with Hadoop supplying its own key. The mapper produces a list of key value pairs.
Both the key and the value may change from the k1 and v1 that it came in as to a k2 and v2.
There can now be duplicate keys coming out of the mappers. The shuffle step will take
care of grouping them together. The output of the shuffle is the input to the reducer
step. Now, we still have a list of the v2&amp;rsquo;s that came out of the mapper step, but they
are grouped by their keys and there is no longer more than one record with the same
key. Finally, coming out of the reducer is, potentially, an entirely new key and value,
k3 and v3. For example, if your reducer summed the values associated with each k2, your k3
would be equal to k2 and your v3 would be the sum of the list of v2s.&lt;/p&gt;

&lt;p&gt;Let us look at an example of a simple data flow. Say we want to transform the input on
the left to the output on the right. On the left, we just have letters. On the right,
we have counts of the number of occurrences of each letter in the input.
Hadoop does the first step for us. It turns the input data into key-value pairs and supplies
its own key: a increasing sequence number. The function we write for the mapper needs
to take these key-value pairs and produce something that the reduce step can use to
count occurrences. The simplest solution is make each letter a key and make every value
a 1. The shuffle groups records having the same
key together, so we see B now has two values, both 1, associated with it.
The reduce is simple: it just sums the values it is given to produce a sum for each key.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/mapreduce_simple_dataflow_example.png&#34; alt=&#34;MapReduce - Simple Data Flow Example&#34; title=&#34;MapReduce - Simple Data Flow Example&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_Pig_Hive/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs/Unit_4_Pig_Hive/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/&#34;&gt;Big Data University- Hadoop 101&lt;/a&gt; &amp;gt;&amp;gt; &lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt; &amp;gt;&amp;gt; &lt;b&gt;Unit_4_Pig_Hive&lt;/b&gt;
&lt;hr&gt;&lt;/p&gt;

&lt;iframe width=&#34;660&#34; height=&#34;371&#34; src=&#34;https://www.youtube.com/embed/_Ae0x1ffYRs&#34; frameborder=&#34;0&#34; allowfullscreen&gt;&lt;/iframe&gt;

&lt;p&gt;Pig and Hive have much in common. They all translate high-level languages into MapReduce
jobs so that the programmer can work at a higher level than he or she would when writing
MapReduce jobs in Java or other lower-level languages supported by Hadoop using Hadoop
streaming. The high level languages offered by Pig and Hive let you write programs
that are much smaller than the equivalent Java code. When you find that you need to
work at a lower level to accomplish something these high-level languages do not support
themselves, you have the option to extend these languages, often by writing user-defined
functions in Java. Interoperability can work both ways since programs written in these
high-level languages can be imbedded inside other languages as well. Finally, since all
these technologies run on top of Hadoop, when they do so, they have the same limitations
with respect to random reads and writes and low-latency queries as Hadoop does.&lt;/p&gt;

&lt;p&gt;Now, let us examine what is unique about each technology, starting with Pig. Pig was developed
at Yahoo Research around 2006 and moved into the Apache Software Foundation in 2007. Pig&amp;rsquo;s
language, called PigLatin, is a data flow language - this is the kind of language in
which you program by connecting things together. Pig can operate on complex data
structures, even those that can have levels of nesting. Unlike SQL, Pig does not require
that the data have a schema, so it is well suited to processing unstructured data. However,
Pig can still leverage the value of a schema if you choose to supply one. Like SQL, PigLatin
is relationally complete, which means it is at least as powerful as relational algebra.
Turing completeness requires looping constructs, an infinite memory model, and conditional
constructs. PigLatin is not Turing complete on its own, but is Turing complete when extended
with User-Defined Functions.&lt;/p&gt;

&lt;p&gt;Hive is a technology developed at Facebook that turns Hadoop into a data warehouse complete with a dialect of SQL for querying. Being
an SQL dialect, HiveQL is a declarative language. Unlike in PigLatin, you do not specify the
data flow, but instead describe the result you want and Hive figures out how to build
a data flow to achieve it. Also unlike Pig, a schema is required, but you are not limited
to one schema. Like PigLatin and SQL, HiveQL on its own is a relationally complete language
but not a Turing complete language. It can be extended through UDFs just like Pig to
be Turing complete.&lt;/p&gt;

&lt;p&gt;Let us examine Pig in detail. Pig consists
of a language and an execution environment. The language is called PigLatin. There are
two choices of execution environment: a local environment and distributed environment. A
local environment is good for testing when you do not have a full distributed Hadoop
environment deployed. You tell Pig to run in the local
environment when you start Pig&amp;rsquo;s command line interpreter by passing it the -x local option.
You tell Pig to run in a distributed environment by passing -x mapreduce instead. Alternatively,
you can start the Pig command line interpreter without any arguments and it will start it
in the distributed environment. There are three different ways to run Pig. You can run
your PigLatin code as a script, just by passing the name of your script
file to the pig command. You can run it interactively through the grunt command line launched using
pig with no script argument. Finally, you can call into Pig from within Java using Pig&amp;rsquo;s
embedded form.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/pig.png&#34; alt=&#34;Pig&#34; title=&#34;Pig&#34; /&gt;&lt;/p&gt;

&lt;p&gt;As mentioned in the overview, Hive is a technology
for turning Hadoop into a data warehouse, complete with an SQL dialect for querying
it. There are three ways to run Hive. You can run it interactively by launching the
hive shell using the hive command with no arguments. You can run a Hive script by passing
the -f option to the hive command along with the path to your script file. Finally, you
can execute a Hive program as one command by passing the -e option to the hive command
followed by your Hive program in quotes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/hive.png&#34; alt=&#34;Hive&#34; title=&#34;Hive&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Big Data University- Hadoop 101</title>
      <link>http://andrewrgoss.com/2017/big-data-university--hadoop-101/</link>
      <pubDate>Sat, 07 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/big-data-university--hadoop-101/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/post/hadoop.png&#34; alt=&#34;Hadoop&#34; title=&#34;Hadoop&#34; /&gt;&lt;br&gt;
&lt;b&gt;Course Code&lt;/b&gt;: BD0111EN&lt;br&gt;
&lt;a href=&#34;https://courses.bigdatauniversity.com/certificates/32dd5caed6994df9a872c70bf6efb7e8&#34; target=&#34;_blank&#34;&gt;Course Certificate&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://bigdatauniversity.com/courses/introduction-to-hadoop&#34; target=&#34;_blank&#34;&gt;Course Link&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://my.imdemocloud.com&#34; target=&#34;_blank&#34;&gt;IBM Analytics Demo Cloud&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;http://andrewrgoss.com/2017/big-data-university--hadoop-101/lesson_transcripts_labs&#34;&gt;Lesson Transcripts/Labs&lt;/a&gt;
&lt;hr&gt;
My goal in taking this course was to expand upon my knowledge around Apache Hadoop, a free, open source, Java-based programming framework. Topics in this course include:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Hadoop&amp;rsquo;s architecture and core components, such as MapReduce and the Hadoop Distributed File System (HDFS).&lt;/li&gt;
&lt;li&gt;Adding/removing nodes from Hadoop clusters, how to check available disk space on each node, and how to modify configuration parameters.&lt;/li&gt;
&lt;li&gt;Other Apache projects that are part of the Hadoop ecosystem, including Pig, Hive, HBase, ZooKeeper, Oozie, Sqoop, Flume, among others.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The course provides video lectures, diagrams, and an IBM cloud sandbox environment to practice administering a Hadoop cluster and performing tasks such as adding and removing nodes. Additionally, the course provides instructions for connecting to the cloud environment via a SFTP client or a SSH client, the latter through which you can practice using the HDFS command line:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://andrewrgoss.com/img/2017/big-data-university--hadoop-101/hdfs_command_line.png&#34; alt=&#34;HDFS Command Line&#34; title=&#34;HDFS Command Line&#34; /&gt;&lt;/p&gt;

&lt;hr&gt;

&lt;h5 id=&#34;course-progress:8867589f317da4ba51ec6005579cfef2&#34;&gt;Course Progress&lt;/h5&gt;

&lt;progress max=&#34;1.0&#34; value=&#34;1.0&#34;&gt;&lt;/progress&gt;

&lt;p&gt;100% - completed 01/30/17.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://courses.bigdatauniversity.com/certificates/32dd5caed6994df9a872c70bf6efb7e8&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View completion certificate&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>We Need Both Data Scientists &amp; Data Engineers</title>
      <link>http://andrewrgoss.com/2016/we-need-both-data-scientists--data-engineers/</link>
      <pubDate>Tue, 27 Dec 2016 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2016/we-need-both-data-scientists--data-engineers/</guid>
      <description>

&lt;p&gt;&lt;sub&gt;&lt;i&gt;written by Neeraj Chadha&lt;/i&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;When it comes to the medical profession, doctors get all the glory. In the world of IoT, it&amp;rsquo;s data scientists who get most of the attention and acclaim. They extract critical intelligence from big data so businesses can make informed decisions on the spot. But they don&amp;rsquo;t do their work in a vacuum. Data scientists can&amp;rsquo;t dazzle their industries without data engineers. These unheralded champions, equivalent to nurses, ensure that big data keeps flowing. As anyone who works in the medical profession will tell you, it&amp;rsquo;s the nurses who keep the hospital running.&lt;/p&gt;

&lt;p&gt;What exactly do data engineers do? They work behind the scenes to design and maintain the networks and software that keep the big data pipeline operating. Like a hospital&amp;rsquo;s nursing staff, data engineers set the stage and keep it running. The roles of data scientists and data engineers can be confusing because they have some overlap. Data engineer and data scientist are not different titles for the same job, however. The two jobs require different skills and experience. Some data scientists can do data engineering. Some data engineers can do data analysis and data visualization.&lt;/p&gt;

&lt;p&gt;The roles do have distinctions, however. For instance, large applications call for the skills of data engineers. Research is a primary focus of the data scientist. Like nurses, data engineers are a special breed. The best have certain personality traits that help them excel: focus, mechanical aptitude, patience and persistence. Good data engineers get down in the trenches. They want to understand how and why data pipelines work&amp;ndash;or don&amp;rsquo;t work. Data engineers need patience and persistence to set things right.&lt;/p&gt;

&lt;p&gt;To do modeling, data scientists need data engineers to gather, store and process data so they can analyze it for insights. Responsible for data management, data engineers handle procedures, guidelines and standards. They develop data-management technologies and software-engineering tools. They design custom software and discover ways to recover from disasters. They improve data reliability, efficiency and quality. User-defined functions and analytics are part of a data engineer&amp;rsquo;s job, too.&lt;/p&gt;

&lt;p&gt;In contrast, data scientists take a big-picture view of things and have a less nuts-and-bolts relationship with data. They handle analytic projects that arise from the needs of the business. Data scientists also take on data-mining architectures, modeling standards, reporting and data methodologies. They manage data-mining-system performance and efficiency, too.&lt;/p&gt;

&lt;p&gt;Because they build and maintain the data pipelines that send information to data scientists, the work of data engineers is very valuable. They can run basic learning models if they understand algorithms. But data scientists tackle business problems that take sophisticated machine-learning algorithms. The best data scientists adapt machine-learning models to meet the changing requirements of the business or agency.&lt;/p&gt;

&lt;h3 id=&#34;tools-for-tough-big-data-challenges:029588ea80b351abbdc001504769805c&#34;&gt;Tools for Tough Big Data Challenges&lt;/h3&gt;

&lt;p&gt;The challenges of database integration and unstructured big data are handled by the data engineers. They must clean up that unstructured data before they pass it to anyone in the organization who needs it. Like nurses who prep patients for surgery, data engineers prepare the foundation for data scientists to work easily with data. They should know data warehousing, database design, data collection and transfer, and coding.&lt;/p&gt;

&lt;p&gt;The part of the data pipeline on which data engineers are focusing on determines which tools they will use. Data engineers at the rear of the pipeline build APIs for data consumption, integrate data sets from external sources and analyze how the data is used to support business growth.&lt;/p&gt;

&lt;p&gt;Although these professionals have many languages to choose from, Python is a good option. Data engineers use it to write code related to data ingestion. Python can talk to any data store, such as NoSQL and RDBMS. Data engineers may have to use big data technologies such as Hadoop and Spark to suggest improvements on the basis of how data is used.&lt;/p&gt;

&lt;p&gt;Data engineers have many tools at their disposal, including the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;NoSQL databases (e.g., Cassandra and MongoDB)&lt;/li&gt;
&lt;li&gt;Hadoop and related tools such as HBase, Hive and Pig&lt;/li&gt;
&lt;li&gt;Pentaho&lt;/li&gt;
&lt;li&gt;VMware&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;data-scientists-and-data-engineers-growth-on-the-horizon:029588ea80b351abbdc001504769805c&#34;&gt;Data Scientists and Data Engineers: Growth on the Horizon&lt;/h3&gt;

&lt;p&gt;A study last year from the Economist Intelligence Unit surveyed 422 executives in the U.S. and Europe. The survey asked them about the digital skills most in demand among industries such as financial services, health care, manufacturing and retail. Forty-three percent of the executives said that in three years, analytics and big data skills will be the most important digital capabilities at their companies.&lt;/p&gt;

&lt;p&gt;As life and business become increasingly data driven, demand for both data engineers and data scientists will continue to rise. Now is the time for data professionals to acquire or build on their skills so they will be well positioned for career advancement and job security.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.datacenterjournal.com/need-data-scientists-data-engineers&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View original article&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Source: &lt;a href=&#34;http://www.datacenterjournal.com&#34; target=_&gt;Data Center Journal&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Engineering on Data Engineer &amp; Software Developer in Boston</title>
    <link>http://andrewrgoss.com/tags/data-engineering/</link>
    <description>Recent content in Data Engineering on Data Engineer &amp; Software Developer in Boston</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <managingEditor>andrewrgoss@gmail.com (Andrew Goss)</managingEditor>
    <webMaster>andrewrgoss@gmail.com (Andrew Goss)</webMaster>
    <lastBuildDate>Fri, 20 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://andrewrgoss.com/tags/data-engineering/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>The Rise of the Data Engineer</title>
      <link>http://andrewrgoss.com/2017/the-rise-of-the-data-engineer/</link>
      <pubDate>Fri, 20 Jan 2017 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2017/the-rise-of-the-data-engineer/</guid>
      <description>

&lt;p&gt;&lt;sub&gt;&lt;i&gt;written by &lt;a href=&#34;https://medium.freecodecamp.com/@maximebeauchemin&#34; target=&#34;_blank&#34;&gt;Maxime Beauchemin&lt;/a&gt;&lt;/i&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;I joined Facebook in 2011 as a &lt;b&gt;business intelligence engineer&lt;/b&gt;. By the time I left in 2013, I was a &lt;b&gt;data engineer&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;I wasn&amp;rsquo;t promoted or assigned to this new role. Instead, Facebook came to realize that the work we were doing transcended classic business intelligence. The role we&amp;rsquo;d created for ourselves was a new discipline entirely.&lt;/p&gt;

&lt;p&gt;My team was at forefront of this transformation. We were developing new skills, new ways of doing things, new tools, and&amp;ndash;more often than not&amp;ndash;turning our backs to traditional methods.&lt;/p&gt;

&lt;p&gt;We were pioneers. We were data engineers!&lt;/p&gt;

&lt;h2 id=&#34;data-engineering:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Data engineering?&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;Data science&lt;/b&gt; as a discipline was going through its adolescence of self-affirming and defining itself. At the same time, &lt;b&gt;data engineering&lt;/b&gt; was the slightly younger sibling, but it was going through something similar. The data engineering discipline took cues from its sibling, while also defining itself in opposition, and finding its own identity.&lt;/p&gt;

&lt;p&gt;Like data scientists, data engineers write code. They&amp;rsquo;re highly analytical, and are interested in data visualization.&lt;/p&gt;

&lt;p&gt;Unlike data scientists&amp;ndash;and inspired by our more mature parent, &lt;b&gt;software engineering&lt;/b&gt;&amp;ndash;data engineers build tools, infrastructure, frameworks, and services. In fact, it&amp;rsquo;s arguable that data engineering is much closer to software engineering than it is to a data science.&lt;/p&gt;

&lt;p&gt;In relation to &lt;b&gt;previously existing roles&lt;/b&gt;, the data engineering field could be thought of as a superset of &lt;b&gt;business intelligence&lt;/b&gt; and &lt;b&gt;data warehousing&lt;/b&gt; that brings more elements from &lt;b&gt;software engineering&lt;/b&gt;. This discipline also integrates specialization around the operation of so called &amp;ldquo;big data&amp;rdquo; distributed systems, along with concepts around the extended Hadoop ecosystem, stream processing, and in computation at scale.&lt;/p&gt;

&lt;p&gt;In smaller companies&amp;ndash;where no &lt;b&gt;data infrastructure&lt;/b&gt; team has yet been formalized&amp;ndash;the data engineering role may also cover the workload around setting up and operating the organization&amp;rsquo;s data infrastructure. This includes tasks like setting up and operating platforms like Hadoop/Hive/HBase, Spark, and the like.&lt;/p&gt;

&lt;p&gt;In smaller environments people tend to use hosted services offered by Amazon or Databricks, or get support from companies like Cloudera or Hortonworks&amp;ndash;which essentially subcontracts the data engineering role to other companies.&lt;/p&gt;

&lt;p&gt;In larger environments, there tends to be specialization and the creation of a formal role to manage this workload, as the need for a data infrastructure team grows. In those organizations, the role of automating some of the data engineering processes falls under the hand of both the data engineering and data infrastructure teams, and it&amp;rsquo;s common for these teams to collaborate to solve higher level problems.&lt;/p&gt;

&lt;p&gt;While the engineering aspect of the role is growing in scope, other aspects of the original business engineering role are becoming secondary. Areas like crafting and maintaining portfolios of reports and dashboards are not a data engineer&amp;rsquo;s primary focus.&lt;/p&gt;

&lt;p&gt;We now have better self-service tooling where analysts, data scientist and the general &amp;ldquo;information worker&amp;rdquo; is becoming more data-savvy and can take care of data consumption autonomously.&lt;/p&gt;

&lt;h2 id=&#34;etl-is-changing:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;ETL is changing&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve also observed a general shift away from drag-and-drop &lt;a href=&#34;https://en.wikipedia.org/wiki/Extract,_transform,_load&#34; target=&#34;_blank&#34;&gt;ETL (Extract Transform and Load)&lt;/a&gt; tools towards a more programmatic approach. Product know-how on platforms like Informatica, IBM Datastage, Cognos, AbInitio or Microsoft SSIS isn&amp;rsquo;t common amongst modern data engineers, and being replaced by more generic software engineering skills along with understanding of programmatic or configuration driven platforms like Airflow, Oozie, Azkabhan or Luigi. It&amp;rsquo;s also fairly common for engineers to develop and manage their own job orchestrator/scheduler.&lt;/p&gt;

&lt;p&gt;There&amp;rsquo;s a multitude of reasons why complex pieces of software are not developed using drag and drop tools: it&amp;rsquo;s that &lt;mark&gt;ultimately &lt;b&gt;code&lt;/b&gt; is the best abstraction there is for software&lt;/mark&gt;. While it&amp;rsquo;s beyond the scope of this article to argue on this topic, it&amp;rsquo;s easy to infer that these same reasons apply to writing ETL as it applies to any other software. Code allows for arbitrary levels of abstractions, allows for all logical operation in a familiar way, integrates well with source control, is easy to version and to collaborate on. The fact that ETL tools evolved to expose graphical interfaces seems like a detour in the history of data processing, and would certainly make for an interesting blog post of its own.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s highlight the fact that the abstractions exposed by traditional ETL tools are off-target. Sure, there&amp;rsquo;s a need to abstract the complexity of data processing, computation and storage. But I would argue that the solution is not to expose ETL primitives (like source/target, aggregations, filtering) into a drag-and-drop fashion. The abstractions needed are of a higher level.&lt;/p&gt;

&lt;p&gt;For example, an example of a needed abstraction in a modern data environment is the configuration for the experiments in an A/B testing framework: &lt;i&gt;what are all the experiment? what are the related treatments? what percentage of users should be exposed? what are the metrics that each experiment expects to affect? when is the experiment taking effect?&lt;/i&gt; In this example, we have a framework that receives precise, high level input, performs complex statistical computation and delivers computed results. We expect that adding an entry for a new experiment will result in extra computation and results being delivered. What is important to note in this example is that the input parameters of this abstraction are not the one offered by a traditional ETL tool, and that a building such an abstraction in a drag and drop interface would not be manageable.&lt;/p&gt;

&lt;p&gt;To a modern data engineer, traditional ETL tools are largely &lt;b&gt;obsolete&lt;/b&gt; because logic cannot be expressed using code. As a result, the abstractions needed cannot be expressed intuitively in those tools. Now knowing that the data engineer&amp;rsquo;s role consist largely of defining ETL, and knowing that a completely new set of tools and methodology is needed, one can argue that this forces the discipline to rebuild itself from the ground up. New stack, new tools, a new set of constraints, and in many cases, a new generation of individuals.&lt;/p&gt;

&lt;h2 id=&#34;data-modeling-is-changing:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Data modeling is changing&lt;/h2&gt;

&lt;p&gt;Typical data modeling techniques&amp;ndash;like the &lt;a href=&#34;https://en.wikipedia.org/wiki/Star_schema&#34; target=&#34;_blank&#34;&gt;star schema&lt;/a&gt;&amp;ndash;which defined our approach to data modeling for the analytics workloads typically associated with data warehouses, are less relevant than they once were. The traditional best practices of data warehousing are loosing ground on a shifting stack. Storage and compute is cheaper than ever, and with the advent of distributed databases that scale out linearly, the scarcer resource is engineering time.&lt;/p&gt;

&lt;p&gt;Here are some changes observed in data modeling techniques:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;b&gt;further denormalization:&lt;/b&gt; maintaining &lt;a href=&#34;http://www.kimballgroup.com/1998/05/surrogate-keys&#34; target=&#34;_blank&#34;&gt;surrogate keys&lt;/a&gt; in dimensions can be tricky, and it makes fact tables less readable. The use of natural, human readable keys and dimension attributes in fact tables is becoming more common, reducing the need for costly joins that can be heavy on distributed databases. Also note that support for encoding and compression in serialization formats like Parquet or ORC, or in database engines like Vertica, address most of the performance loss that would normally be associated with denormalization. Those systems have been taught to normalize the data for storage on their own.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;blobs:&lt;/b&gt; modern databases have a growing support for blobs through native types and functions. This opens new moves in the data modeler&amp;rsquo;s playbook, and can allow for fact tables to store multiple grains at once when needed&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;dynamic schemas:&lt;/b&gt; since the advent of map reduce, with the growing popularity of document stores and with support for blobs in databases, it&amp;rsquo;s becoming easier to evolve database schemas without executing &lt;a href=&#34;https://en.wikipedia.org/wiki/Data_definition_language&#34; target=&#34;_blank&#34;&gt;DML&lt;/a&gt;. This makes it easier to have an iterative approach to warehousing, and removes the need to get full consensus and buy-in prior to development.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;systematically &lt;b&gt;snapshoting&lt;/b&gt; dimensions (storing a full copy of the dimension for each ETL schedule cycle, usually in distinct table partitions) as a generic way to handle &lt;a href=&#34;https://en.wikipedia.org/wiki/Slowly_changing_dimension&#34; target=&#34;_blank&#34;&gt;slowly changing dimension&lt;/a&gt; (SCD) is a simple generic approach that requires little engineering effort, and that unlike the classical approach, is easy to grasp when writing ETL and queries alike. It&amp;rsquo;s also easy and relatively cheap to denormalize the dimension&amp;rsquo;s attribute into the fact table to keep track of its value at the moment of the transaction. In retrospect, complex SCD modeling techniques are not intuitive and reduce accessibility.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;b&gt;conformance&lt;/b&gt;, as in &lt;a href=&#34;http://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dimensional-modeling-techniques/conformed-dimension&#34; target=&#34;_blank&#34;&gt;conformed dimensions&lt;/a&gt; and metrics is still extremely important in modern data environment, but with the need for data warehouses to move fast, and with more team and roles invited to contribute to this effort, it&amp;rsquo;s less imperative and more of a tradeoff. Consensus and convergence can happen as a background process in the areas where the pain point of divergence become out-of-hand.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Also, more generally, it&amp;rsquo;s arguable to say that with the commoditization of compute cycles and with more people being data-savvy then before, there&amp;rsquo;s less need to precompute and store results in the warehouse. For instance you can have complex Spark job that can compute complex analysis on-demand only, and not be scheduled to be part of the warehouse.&lt;/p&gt;

&lt;h2 id=&#34;roles-responsibilities:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Roles &amp;amp; responsibilities&lt;/h2&gt;

&lt;h3 id=&#34;the-data-warehouse:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;The data warehouse&lt;/h3&gt;

&lt;p&gt;&lt;i&gt;A data warehouse is a copy of transaction data specifically structured for query and analysis.&lt;/i&gt;  &amp;ndash;Ralph Kimball&lt;/p&gt;

&lt;p&gt;&lt;i&gt;A data warehouse is a subject-oriented, integrated, time-variant and non-volatile collection of data in support of management&amp;rsquo;s decision making process.&lt;/i&gt;  &amp;ndash;Bill Inmon&lt;/p&gt;

&lt;p&gt;The data warehouse is just as relevant as it ever was, and data engineers are in charge of many aspects of its construction and operation. The data engineer&amp;rsquo;s focal point is the data warehouse and gravitates around it.&lt;/p&gt;

&lt;p&gt;The modern data warehouse is a more public institution than it was historically, welcoming data scientists, analysts, and software engineers to partake in its construction and operation. Data is simply too centric to the company&amp;rsquo;s activity to have limitation around what roles can manage its flow. While this allows scaling to match the organization&amp;rsquo;s data needs, it often results in a much more chaotic, shape-shifting, imperfect piece of infrastructure.&lt;/p&gt;

&lt;p&gt;The data engineering team will often own pockets of certified, high quality areas in the data warehouse. At Airbnb for instance, there&amp;rsquo;s a set of &amp;ldquo;core&amp;rdquo; schemas that are managed by the data engineering team, where service level agreement (SLAs) are clearly defined and measured, naming conventions are strictly followed, business metadata and documentation is of the highest quality, and the related pipeline code follows a set of well defined best practices.&lt;/p&gt;

&lt;p&gt;It also becomes the role of the data engineering team to be a &amp;ldquo;center of excellence&amp;rdquo; through the definitions of standards, best practices and certification processes for data objects. The team can evolve to partake or lead an education program sharing its core competencies to help other teams become better citizens of the data warehouse. For instance, Facebook has a &amp;ldquo;data camp&amp;rdquo; education program and Airbnb is developing a similar &amp;ldquo;Data University&amp;rdquo; program where data engineers lead session that teach people how to be proficient with data.&lt;/p&gt;

&lt;p&gt;Data engineers are also the &amp;ldquo;librarians&amp;rdquo; of the data warehouse, cataloging and organizing metadata, defining the processes by which one files or extract data from the warehouse. In a fast growing, rapidly evolving, slightly chaotic data ecosystem, metadata management and tooling become a vital component of a modern data platform.&lt;/p&gt;

&lt;h3 id=&#34;performance-tuning-and-optimization:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Performance tuning and optimization&lt;/h3&gt;

&lt;p&gt;With data becoming more strategic than ever, companies are growing impressive budgets for their data infrastructure. This makes it increasingly rational for data engineers to spend cycles on performance tuning and optimization of data processing and storage. Since the budgets are rarely shrinking in this area, optimization is often coming from the perspective of achieving more with the same amount of resources or trying to linearize exponential growth in resource utilization and costs.&lt;/p&gt;

&lt;p&gt;Knowing that the complexity of the data engineering stack is exploding we can assume that the complexity of optimizing such stack and processes can be just as challenging. Where it can be easy to get huge wins with little effort, diminishing returns laws typically apply.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s definitely in the interest of the data engineer to build [on] infrastructure that scales with the company, and to be resource conscious at all times.&lt;/p&gt;

&lt;h4 id=&#34;data-integration:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Data Integration&lt;/h4&gt;

&lt;p&gt;Data integration, the practice behind integrating businesses and systems
through the exchange of data, is as important and as challenging as its ever
been. As &lt;a href=&#34;https://en.wikipedia.org/wiki/Software_as_a_service&#34; target=&#34;_blank&#34;&gt;Software as a Service (SaaS)&lt;/a&gt; becomes the new standard way for companies to operate, the need to synchronize referential data across these systems becomes increasingly critical. Not only SaaS needs up-to-date data to function, we often want to bring the data generated on their side into our data warehouse so that it can be analyzed along the rest of our data. Sure SaaS often have their own analytics offering, but are systematically lacking the perspective that the rest of you company&amp;rsquo;s data offer, so more often than not it&amp;rsquo;s necessary to pull some of this data back.&lt;/p&gt;

&lt;p&gt;Letting these SaaS offering redefine referential data without integrating and sharing a common primary key is a disaster that should be avoided at all costs. No one wants to manually maintain two employee or customer lists in 2 different systems, and worst: having to do fuzzy matching when bringing their HR data back into their warehouse.&lt;/p&gt;

&lt;p&gt;Worst, company executive often sign deal with SaaS providers without really
considering the data integration challenges. The integration workload is systematically downplayed by vendors to facilitate their sales, and leaves data engineers stuck doing unaccounted, under appreciated work to do. Let alone the fact that typical SaaS APIs are often poorly designed, unclearly documented and &amp;ldquo;agile&amp;rdquo;: meaning that you can expect them to change without notice.&lt;/p&gt;

&lt;h3 id=&#34;services:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Services&lt;/h3&gt;

&lt;p&gt;Data engineers are operating at a higher level of abstraction and in some cases that means providing services and tooling to automate the type of work that data engineers, data scientists or analysts may do manually.&lt;/p&gt;

&lt;p&gt;Here are a few examples of services that data engineers and data infrastructure engineer may build and operate.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;data ingestion: services and tooling around &amp;ldquo;scraping&amp;rdquo; databases, loading logs, fetching data from external stores or APIs, &amp;hellip;&lt;/li&gt;
&lt;li&gt;metric computation: frameworks to compute and summarize engagement, growth or segmentation related metrics&lt;/li&gt;
&lt;li&gt;anomaly detection: automating data consumption to alert people anomalous events occur or when trends are changing significantly&lt;/li&gt;
&lt;li&gt;metadata management: tooling around allowing generation and consumption of metadata, making it easy to find information in and around the data warehouse&lt;/li&gt;
&lt;li&gt;experimentation: A/B testing and experimentation frameworks is often a critical piece of company&amp;rsquo;s analytics with a significant data engineering component to it&lt;/li&gt;
&lt;li&gt;instrumentation: analytics starts with logging events and attributes related to those events, data engineers have vested interests in making sure that high quality data is captured upstream&lt;/li&gt;
&lt;li&gt;sessionization: pipelines that are specialized in understand series of actions in time, allowing analysts to understand user behaviors&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Just like software engineers, data engineers should be constantly looking to automate their workloads and building abstraction that allow them to climb the complexity ladder. While the nature of the workflows that can be automated differs depending on the environment, the need to automate them is common across the board.&lt;/p&gt;

&lt;h2 id=&#34;required-skills:6a9fa2d276b1eefe8e0ddcdbde3e2acf&#34;&gt;Required Skills&lt;/h2&gt;

&lt;p&gt;&lt;b&gt;SQL mastery:&lt;/b&gt; if english is the language of business, SQL is the language of data. How successful of a business man can you be if you don&amp;rsquo;t speak good english? While generations of technologies age and fade, SQL is still standing strong as the lingua franca of data. A data engineer should be able to express any degree of complexity in SQL using techniques like &amp;ldquo;correlated subqueries&amp;rdquo; and window functions. SQL/DML/DDL primitives are simple enough that it should hold no secrets to a data engineer. Beyond the declarative nature of SQL, she/he should be able to read and understand database execution plans, and have an understanding of what all the steps are, how indices work, the different join algorithm and the distributed dimension within the plan.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Data modeling techniques:&lt;/b&gt; for a data engineer, entity-relationship modeling should be a cognitive reflex, along with a clear understanding of normalization, and have a sharp intuition around denormalization tradeoffs. The data engineer should be familiar with &lt;a href=&#34;https://en.wikipedia.org/wiki/Dimensional_modeling&#34; target=&#34;_blank&#34;&gt;dimensional modeling&lt;/a&gt; and the related concepts and lexical field.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;ETL design:&lt;/b&gt; writing efficient, resilient and &amp;ldquo;evolvable&amp;rdquo; ETL is key. This topic will be expanded upon in an upcoming blog post.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Architectural projections:&lt;/b&gt; like any professional in any given field of expertise, the data engineer needs to have a high level understanding of most of the tools, platforms, libraries and other resources at its disposal. The properties, use-cases and subtleties behind the different flavors of databases, computation engines, stream processors, message queues, workflow orchestrators, serialization formats and other related technologies. When designing solutions, she/he should be able to make good choices as to which technologies to use and have a vision as to how to make them work together.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://medium.freecodecamp.com/the-rise-of-the-data-engineer-91be18f1e603&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View original article&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Source: &lt;a href=&#34;https://medium.freecodecamp.com&#34; target=_&gt;FreeCodeCamp&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>We Need Both Data Scientists &amp; Data Engineers</title>
      <link>http://andrewrgoss.com/2016/we-need-both-data-scientists--data-engineers/</link>
      <pubDate>Tue, 27 Dec 2016 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2016/we-need-both-data-scientists--data-engineers/</guid>
      <description>

&lt;p&gt;&lt;sub&gt;&lt;i&gt;written by Neeraj Chadha&lt;/i&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;When it comes to the medical profession, doctors get all the glory. In the world of IoT, it&amp;rsquo;s data scientists who get most of the attention and acclaim. They extract critical intelligence from big data so businesses can make informed decisions on the spot. But they don&amp;rsquo;t do their work in a vacuum. Data scientists can&amp;rsquo;t dazzle their industries without data engineers. These unheralded champions, equivalent to nurses, ensure that big data keeps flowing. As anyone who works in the medical profession will tell you, it&amp;rsquo;s the nurses who keep the hospital running.&lt;/p&gt;

&lt;p&gt;What exactly do data engineers do? They work behind the scenes to design and maintain the networks and software that keep the big data pipeline operating. Like a hospital&amp;rsquo;s nursing staff, data engineers set the stage and keep it running. The roles of data scientists and data engineers can be confusing because they have some overlap. Data engineer and data scientist are not different titles for the same job, however. The two jobs require different skills and experience. Some data scientists can do data engineering. Some data engineers can do data analysis and data visualization.&lt;/p&gt;

&lt;p&gt;The roles do have distinctions, however. For instance, large applications call for the skills of data engineers. Research is a primary focus of the data scientist. Like nurses, data engineers are a special breed. The best have certain personality traits that help them excel: focus, mechanical aptitude, patience and persistence. Good data engineers get down in the trenches. They want to understand how and why data pipelines work&amp;ndash;or don&amp;rsquo;t work. Data engineers need patience and persistence to set things right.&lt;/p&gt;

&lt;p&gt;To do modeling, data scientists need data engineers to gather, store and process data so they can analyze it for insights. Responsible for data management, data engineers handle procedures, guidelines and standards. They develop data-management technologies and software-engineering tools. They design custom software and discover ways to recover from disasters. They improve data reliability, efficiency and quality. User-defined functions and analytics are part of a data engineer&amp;rsquo;s job, too.&lt;/p&gt;

&lt;p&gt;In contrast, data scientists take a big-picture view of things and have a less nuts-and-bolts relationship with data. They handle analytic projects that arise from the needs of the business. Data scientists also take on data-mining architectures, modeling standards, reporting and data methodologies. They manage data-mining-system performance and efficiency, too.&lt;/p&gt;

&lt;p&gt;Because they build and maintain the data pipelines that send information to data scientists, the work of data engineers is very valuable. They can run basic learning models if they understand algorithms. But data scientists tackle business problems that take sophisticated machine-learning algorithms. The best data scientists adapt machine-learning models to meet the changing requirements of the business or agency.&lt;/p&gt;

&lt;h3 id=&#34;tools-for-tough-big-data-challenges:029588ea80b351abbdc001504769805c&#34;&gt;Tools for Tough Big Data Challenges&lt;/h3&gt;

&lt;p&gt;The challenges of database integration and unstructured big data are handled by the data engineers. They must clean up that unstructured data before they pass it to anyone in the organization who needs it. Like nurses who prep patients for surgery, data engineers prepare the foundation for data scientists to work easily with data. They should know data warehousing, database design, data collection and transfer, and coding.&lt;/p&gt;

&lt;p&gt;The part of the data pipeline on which data engineers are focusing on determines which tools they will use. Data engineers at the rear of the pipeline build APIs for data consumption, integrate data sets from external sources and analyze how the data is used to support business growth.&lt;/p&gt;

&lt;p&gt;Although these professionals have many languages to choose from, Python is a good option. Data engineers use it to write code related to data ingestion. Python can talk to any data store, such as NoSQL and RDBMS. Data engineers may have to use big data technologies such as Hadoop and Spark to suggest improvements on the basis of how data is used.&lt;/p&gt;

&lt;p&gt;Data engineers have many tools at their disposal, including the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Spark&lt;/li&gt;
&lt;li&gt;NoSQL databases (e.g., Cassandra and MongoDB)&lt;/li&gt;
&lt;li&gt;Hadoop and related tools such as HBase, Hive and Pig&lt;/li&gt;
&lt;li&gt;Pentaho&lt;/li&gt;
&lt;li&gt;VMware&lt;/li&gt;
&lt;li&gt;JavaScript&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;data-scientists-and-data-engineers-growth-on-the-horizon:029588ea80b351abbdc001504769805c&#34;&gt;Data Scientists and Data Engineers: Growth on the Horizon&lt;/h3&gt;

&lt;p&gt;A study last year from the Economist Intelligence Unit surveyed 422 executives in the U.S. and Europe. The survey asked them about the digital skills most in demand among industries such as financial services, health care, manufacturing and retail. Forty-three percent of the executives said that in three years, analytics and big data skills will be the most important digital capabilities at their companies.&lt;/p&gt;

&lt;p&gt;As life and business become increasingly data driven, demand for both data engineers and data scientists will continue to rise. Now is the time for data professionals to acquire or build on their skills so they will be well positioned for career advancement and job security.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.datacenterjournal.com/need-data-scientists-data-engineers&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View original article&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Source: &lt;a href=&#34;http://www.datacenterjournal.com&#34; target=_&gt;Data Center Journal&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Data Science Industry: Who Does What</title>
      <link>http://andrewrgoss.com/2016/the-data-science-industry-who-does-what/</link>
      <pubDate>Wed, 09 Nov 2016 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2016/the-data-science-industry-who-does-what/</guid>
      <description>&lt;p&gt;Great infographic from &lt;a href=&#34;https://www.datacamp.com&#34; target=&#34;_blank&#34;&gt;DataCamp&lt;/a&gt; that lays out the roles and skills associated with a data science team. As a data engineer within this group, these are the kind of people I work with day-to-day.
&lt;img src=&#34;http://101.datascience.community/wp-content/uploads/2015/11/datasciencejobs.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Source: &lt;a href=&#34;https://www.datacamp.com&#34; target=&#34;_blank&#34;&gt;DataCamp&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data Engineers vs. Data Scientists</title>
      <link>http://andrewrgoss.com/2016/data-engineers-vs.-data-scientists/</link>
      <pubDate>Thu, 29 Sep 2016 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2016/data-engineers-vs.-data-scientists/</guid>
      <description>&lt;p&gt;Data engineer and data scientist are the two most popular career tracks in big data - but how do they compare? At a high level, data engineers are concerned with data infrastructure, while data scientists are concerned with analysis.&lt;/p&gt;

&lt;p&gt;Like data scientists, data engineers write code. They’re highly analytical, and are interested in data visualization.
Unlike data scientists — and inspired by a more mature parent, software engineering — data engineers build tools, infrastructure, frameworks, and services. In fact, it’s arguable that data engineering is much closer to software engineering than it is to data science.&lt;/p&gt;

&lt;p&gt;This article by &lt;a href=&#34;https://www.stitchdata.com&#34; target=_&gt;Stitch&lt;/a&gt; digs into LinkedIn data to analyze the skills associated with each role. It also provides links to other helpful resources for further perspective.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/1*QOw6_pX9r1G_ua_n0DsICQ.png&#34;&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://blog.stitchdata.com/data-engineer-vs-data-scientist-the-difference-according-to-linkedin-e0bee6248fd4&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View the article&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Sources: &lt;a href=&#34;https://www.stitchdata.com&#34; target=_&gt;Stitch&lt;/a&gt;, &lt;a href=&#34;https://medium.freecodecamp.com&#34; target=_&gt;FreeCodeCamp&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Report- The State of Data Engineering</title>
      <link>http://andrewrgoss.com/2016/report--the-state-of-data-engineering/</link>
      <pubDate>Wed, 28 Sep 2016 00:00:00 +0000</pubDate>
      <author>andrewrgoss@gmail.com (Andrew Goss)</author>
      <guid>http://andrewrgoss.com/2016/report--the-state-of-data-engineering/</guid>
      <description>&lt;p&gt;I recently came across an excellent report by &lt;a href=&#34;https://www.stitchdata.com&#34; target=_&gt;Stitch&lt;/a&gt; on the state of data engineering.&lt;/p&gt;

&lt;p&gt;Today, there are 6,500 people on LinkedIn who call themselves data engineers. In San Francisco alone, there are 6,600 job listings for this same title. The number of data engineers has doubled in the past year, but engineering leaders still find themselves faced with a significant shortage of data engineering talent.&lt;/p&gt;

&lt;p&gt;The need for data talent is born from a fundamental shift: tech companies are now data companies. Uber, AirBnB, Spotify–these companies build data products, and as a result, are scrambling to hire (and hold onto) the people that build and maintain data systems. Josh Wills, Data Engineer at Slack, half-joked, half-pleaded at DataEngConf 2016, &amp;ldquo;Please don&amp;rsquo;t hire my data engineers, they are all here now.&amp;rdquo; Even Slack, one of the hottest tech companies in the valley, is worried about holding onto this valuable talent.&lt;/p&gt;

&lt;p&gt;This report takes an in-depth look at the following:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The number of data engineers in the market today&lt;/li&gt;
&lt;li&gt;Their backgrounds and core skills—information that is particularly valuable for leaders thinking about how to transition software engineers into data engineering roles.&lt;/li&gt;
&lt;li&gt;Employment information that can help make the case for investing in this often expensive skill set.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Answers to these questions are paired with input from engineering leaders at Stripe, MIT, Looker, and more; who share their strategies for finding and retaining talent, developing data engineering talent in-house, and prioritizing a data engineering team&amp;rsquo;s projects. This report presents a clear snapshot of the current state of data engineering.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.stitchdata.com/resources/reports/the-state-of-data-engineering&#34; class=&#34;btn&#34; target=&#34;_blank&#34;&gt;View the full report&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;*Source: &lt;a href=&#34;https://www.stitchdata.com&#34; target=_&gt;Stitch&lt;/a&gt;&lt;/sub&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>